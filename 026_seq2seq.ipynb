{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RNN 망을 이용해서 마지막글자를 예측하는것\n",
    "    #텍스트는 일단 embadding해주어야 한다./onehotencoding\n",
    "    #multirnncell : rnn망의 레이어를 구축\n",
    "    #지도학습 문제로 풀어서 앞의 3글자는 글자를 학습하고 뒤의 한글자는 target으로 예측하도록 학습하였다.\n",
    "    \n",
    "#문자인 경우 모든 문서에있는 단어와 취합된다. 모든문자를 인덱싱->숫자로 변경\n",
    "    #정밀한 rnn을 위해서 layer를 줘서(이용해서) 학습한다. (여러층 쌓는게 가능하다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph() #그래프 초기화\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "#사전\n",
    "char_arr = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z'] #총 26개\n",
    "num_dic = {n:i for i, n in enumerate(char_arr)} #a:0, b:1, c:2 ... 인덱스 번호로 매핑됨, 딕셔러니 자료구조\n",
    "dic_len = len(num_dic)\n",
    "seq_data = ['word','wood','deep','dive','cold','door','load','data','belt','kind']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(seq_data):\n",
    "    #one hot 인코딩을 위한 함수이다.\n",
    "    input_batch = []\n",
    "    target_batch = []\n",
    "    for seq in seq_data:\n",
    "        #word 가 처음으로 들어옴\n",
    "        input = [num_dic[n] for n in seq[:-1]] #wor이 들어옴 [22,14,17]\n",
    "        target = num_dic[seq[-1]] #d가 들어옴 = 3\n",
    "        input_batch.append(np.eye(dic_len)[input]) #np.eye(가로세로 26개인 행렬생성) , eye=단위행렬(대각선으로 1인행렬)\n",
    "        #여기서 앞의 3글자가 원핫인코딩이 된다.\n",
    "        target_batch.append(target)\n",
    "    return input_batch, target_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "n_hidden = 128 #neurons개수 - 출력차수와 같은 개념\n",
    "total_epoch = 30\n",
    "n_step = 3 #셀의 갯수(3줄을 가지기 떄문에, 3개의 글자)\n",
    "n_input = n_class = dic_len #26개\n",
    "X = tf.placeholder(tf.float32, [None, n_step, n_input]) #none, 3, 26개\n",
    "Y = tf.placeholder(tf.int32, [None]) #0~25\n",
    "W = tf.Variable(tf.random_normal([n_hidden, n_class])) #128#26개의 분류로 나온다.\n",
    "b = tf.Variable(tf.random_normal([n_class])) #26 출력차수와 동일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell1 = tf.nn.rnn_cell.BasicLSTMCell(n_hidden) #역전파를 앞단까지 설명하는 lstm\n",
    "#과적합 방지를 위해 dropout을 사용하고 있다. (0.5만 남기고 반을 계산하지 마라.)\n",
    "cell1 = tf.nn.rnn_cell.DropoutWrapper(cell1, output_keep_prob=0.5, seed=0.5)\n",
    "cell2 = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)\n",
    "multi_cell = tf.nn.rnn_cell.MultiRNNCell([cell1, cell2]) #층을 더 깊게 쌓는 multi rnn cell\n",
    "#dynamic cell에 입력하기 전에 layer를 먼저 구성하였다.\n",
    "outputs, states = tf.nn.dynamic_rnn(multi_cell, X, dtype=tf.float32) #메모리 공간을 가변적으로 설정하는 dynamic\n",
    "#(static하게 메모리공간을 미리 확보하지 않고 계산수 버리는 구조로 가변적이다.)\n",
    "#lstm cell을 이용해서 rnn망을 구성하고 있다.(output3개)\n",
    "#output 10*3*128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = tf.transpose(outputs, [1,0,2]) #3(step수)*10(데이터개수)*128(뉴런수) , 각셀마다 output이 나옴\n",
    "outputs = outputs[-1] #10*128 (맨마지막 셀에서 들어온 10개의 128개 = 마지막 state)\n",
    "model = tf.matmul(outputs, W)+b #output = 10*128 , W = 128*26 -> 10*26(원핫 인코딩 되어서 나가는 값과 같다.)\n",
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=model, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  0001 , cost :  4.311453\n",
      "Epoch :  0002 , cost :  3.340565\n",
      "Epoch :  0003 , cost :  1.917046\n",
      "Epoch :  0004 , cost :  1.628191\n",
      "Epoch :  0005 , cost :  0.955354\n",
      "Epoch :  0006 , cost :  0.331631\n",
      "Epoch :  0007 , cost :  0.520275\n",
      "Epoch :  0008 , cost :  0.224936\n",
      "Epoch :  0009 , cost :  0.219465\n",
      "Epoch :  0010 , cost :  0.064895\n",
      "Epoch :  0011 , cost :  0.043829\n",
      "Epoch :  0012 , cost :  0.068688\n",
      "Epoch :  0013 , cost :  0.052668\n",
      "Epoch :  0014 , cost :  0.003880\n",
      "Epoch :  0015 , cost :  0.006514\n",
      "Epoch :  0016 , cost :  0.007736\n",
      "Epoch :  0017 , cost :  0.000534\n",
      "Epoch :  0018 , cost :  0.002623\n",
      "Epoch :  0019 , cost :  0.000947\n",
      "Epoch :  0020 , cost :  0.001914\n",
      "Epoch :  0021 , cost :  0.000803\n",
      "Epoch :  0022 , cost :  0.001008\n",
      "Epoch :  0023 , cost :  0.001099\n",
      "Epoch :  0024 , cost :  0.001565\n",
      "Epoch :  0025 , cost :  0.014804\n",
      "Epoch :  0026 , cost :  0.001142\n",
      "Epoch :  0027 , cost :  0.000923\n",
      "Epoch :  0028 , cost :  0.000172\n",
      "Epoch :  0029 , cost :  0.000147\n",
      "Epoch :  0030 , cost :  0.000241\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer()) #변수를 초기화 해주어야함 필수!\n",
    "input_batch, target_batch = make_batch(seq_data) #원핫인코딩, 숫자로 변환된값\n",
    "for epoch in range(total_epoch):\n",
    "    _,loss = sess.run([optimizer, cost], feed_dict={X:input_batch, Y:target_batch})\n",
    "    print('Epoch : ', '%04d'%(epoch+1), ', cost : ', '{:.6f}'.format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측데이터 [ 3  3 15  4  3 17  3  0 19  3]\n",
      "정확도 1.0\n",
      "예측결과\n",
      "입력값 :  ['wor ', 'woo ', 'dee ', 'div ', 'col ', 'doo ', 'loa ', 'dat ', 'bel ', 'kin ']\n",
      "예측값 :  ['word', 'wood', 'deep', 'dive', 'cold', 'door', 'load', 'data', 'belt', 'kind']\n",
      "정확도 :  1.0\n"
     ]
    }
   ],
   "source": [
    "#모델 결과값 10*26 ->10*1\n",
    "prediction = tf.cast(tf.argmax(model, 1), tf.int32) #argmax 그중 인덱스값을 취한값이 10개가 나온다.\n",
    "prediction_check = tf.equal(prediction, Y)\n",
    "accuracy = tf.reduce_mean(tf.cast(prediction_check, tf.float32)) #casting\n",
    "input_batch, target_batch = make_batch(seq_data)\n",
    "predict, accuracy_val = sess.run([prediction, accuracy], feed_dict={X:input_batch, Y:target_batch}) #예측된값의 accuracy\n",
    "print('예측데이터', predict)\n",
    "print('정확도', accuracy_val)\n",
    "\n",
    "predict_words=[]\n",
    "for idx, val in enumerate(seq_data):\n",
    "    #idx 인덱스 val 문자열\n",
    "    last_char = char_arr[predict[idx]]\n",
    "    predict_words.append(val[:3]+last_char) #마지막놈 더해줌\n",
    "print('예측결과')\n",
    "print('입력값 : ', [w[:3]+' ' for w in seq_data])\n",
    "print('예측값 : ',predict_words)\n",
    "print('정확도 : ',accuracy_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['대한민국', '의', '무궁', '한', '발전', '과', '세계', '를', '이끄는', '지도', '국가', '가', '되', '기를', '희망', '합니다', '.']\n"
     ]
    }
   ],
   "source": [
    "#java위에서 만들어진 패키지가 존재 - JAVA_NOME을 설정해주어야 한다.\n",
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "token = okt.morphs('대한민국의 무궁한 발전과 세계를 이끄는 지도국가가 되기를 희망합니다.')\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#영어 nltk, 한글 konlpy - 한글처리 패키지를 모아서 구성\n",
    "    #한글 파싱조건은 공백을 기준으로 해서 파싱한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'대한민국': 0, '의': 1, '무궁': 2, '한': 3, '발전': 4, '과': 5, '세계': 6, '를': 7, '이끄는': 8, '지도': 9, '국가': 10, '가': 11, '되': 12, '기를': 13, '희망': 14, '합니다': 15, '.': 16}\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "word2index={}\n",
    "for voca in token:\n",
    "    if voca not in word2index.keys():\n",
    "        word2index[voca]=len(word2index) #길이를 재게되면 인덱스가 된다.\n",
    "print(word2index)\n",
    "#인덱싱된다.(번호를 매긴다.)\n",
    "print(len(word2index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encoding(word, word2index):\n",
    "    #원핫 인코딩\n",
    "    one_hot_vector = [0]*(len(word2index)) #단어만큼의 0이 만들어진다.(17개)\n",
    "    index = word2index[word] #몇번에 있느냐\n",
    "    one_hot_vector[index] = 1\n",
    "    return one_hot_vector\n",
    "print(one_hot_encoding('대한민국', word2index)) #있는곳에 1을 집어넣어라\n",
    "print(one_hot_encoding('국가', word2index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#번역기\n",
    "#seq2seq\n",
    "    #attention 으로 중요한 단어 집중망 - NMT발전\n",
    "    #rnn 을 제거하고 self-Attention\n",
    "    #Transformer\n",
    "    #BERT(2018년 말) -> 변종들이 작년에 등장\n",
    "#vector화는 gensim으로 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "char_arr = [c for c in 'SEPabcdefghijklmnopqrstuvwxyz단어나무게임벨트놀이자료'] #사전식으로 여기있는놈만 사용하겠다.\n",
    "num_dic = {n:i for i,n in enumerate(char_arr)} #{'S':0, 'E':1 ...} 형태로 dictionary가 만들어진다. (41개)\n",
    "\n",
    "dic_len = len(num_dic)\n",
    "print(dic_len)\n",
    "seq_data = [['word','단어'],['wood','나무'],['game','게임'],['belt','벨트'],['play','놀이'],['data','자료']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(seq_data):\n",
    "    input_batch=[] #6*4*41\n",
    "    output_batch=[] #6*3*41\n",
    "    target_batch=[] #6*3*41\n",
    "    for seq in seq_data:\n",
    "        #word가 처음으로 들어간다.\n",
    "        input_ = [num_dic[n] for n in seq[0]] #w,o,r,d = [26,18,21,6], smoking gun이 된다.\n",
    "        output = [num_dic[n] for n in ('S'+seq[1])] #S(sos, start),단,어 = [0,30,31] \n",
    "        target = [num_dic[n] for n in (seq[1]+'E')] #어-> 예측할 놈없다 E(end), 종료지점은 확인할 수 있다.(e가 예측이되면 끝나는것)\n",
    "        \n",
    "        #26,18,21,6 이 만들어내는 숫자가 다음셀의 시작이 된다.\n",
    "        \n",
    "        #망이 2개이기 때문에 3개가 있다.\n",
    "        input_batch.append(np.eye(dic_len)[input_]) #41*41 단위행렬(대각선값 1)을 만든다. 4*41의 행렬이 추출된다.\n",
    "        output_batch.append(np.eye(dic_len)[output]) #3*41 (첫번째 무조건 0,30,31) \n",
    "        target_batch.append(target) #30,31,1 , 3*41\n",
    "    return input_batch, output_batch, target_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x0000027DA8A8AF08>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x0000027DA8A8AF08>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x0000027DA8A8AF08>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x0000027DA8A8AF08>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x0000027DA8A8AC08>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x0000027DA8A8AC08>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x0000027DA8A8AC08>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x0000027DA8A8AC08>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000027DA91571C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000027DA91571C8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000027DA91571C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000027DA91571C8>>: AttributeError: module 'gast' has no attribute 'Index'\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "n_hidden = 128\n",
    "total_epoch = 100\n",
    "n_class = n_input = dic_len #41\n",
    "enc_input = tf.placeholder(tf.float32, [None, None, n_input]) #6(데이터갯수)*4(글자갯수, cell의 갯수)*41\n",
    "dec_input = tf.placeholder(tf.float32, [None, None, n_input]) #6(데이터갯수)*3(아웃풋의 갯수)*41\n",
    "targets = tf.placeholder(tf.int64, [None, None]) #6(데이터갯수)*3(아웃풋의 갯수)*41\n",
    "\n",
    "with tf.variable_scope('encoded'):\n",
    "    #인코드망 - 영어를 학습\n",
    "    enc_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden) #128개의 뉴런가지고 있음 = 출력차수\n",
    "    enc_cell = tf.nn.rnn_cell.DropoutWrapper(enc_cell, output_keep_prob=0.5, seed=100) #계산만 줄임\n",
    "    outputs, enc_states = tf.nn.dynamic_rnn(enc_cell, enc_input, dtype=tf.float32) # 6*4*128(뉴런수), 6*128(state)\n",
    "with tf.variable_scope('decoded'):\n",
    "    #디코드망 - 한글을 학습\n",
    "    #target을 예측한다. \n",
    "    dec_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n",
    "    dec_cell = tf.nn.rnn_cell.DropoutWrapper(dec_cell, output_keep_prob=0.5)\n",
    "    outputs, dec_states = tf.nn.dynamic_rnn(dec_cell, dec_input, initial_state=enc_states, dtype=tf.float32) #6*3*128, 6*128(state)\n",
    "    #앞단의 enc_state를 넣어주어 연결하였다.\n",
    "    #state - 셀과 셀을 연결, 망과망을 연결하여 중요한 역할을 한다.(학습시 다음망으로 연결하여 지속적으로 재사용 될 수 있도록한다.)\n",
    "        #학습속도 향상등 많은 역할을 한다.\n",
    "model = tf.layers.dense(outputs, n_class, activation=None)  #target 41개\n",
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=model, labels=targets))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  0001 , Cost :  3.775191\n",
      "Epoch :  0002 , Cost :  2.636400\n",
      "Epoch :  0003 , Cost :  1.493256\n",
      "Epoch :  0004 , Cost :  1.211722\n",
      "Epoch :  0005 , Cost :  0.607132\n",
      "Epoch :  0006 , Cost :  0.437810\n",
      "Epoch :  0007 , Cost :  0.217722\n",
      "Epoch :  0008 , Cost :  0.292883\n",
      "Epoch :  0009 , Cost :  0.123105\n",
      "Epoch :  0010 , Cost :  0.077608\n",
      "Epoch :  0011 , Cost :  0.120656\n",
      "Epoch :  0012 , Cost :  0.182529\n",
      "Epoch :  0013 , Cost :  0.057068\n",
      "Epoch :  0014 , Cost :  0.117649\n",
      "Epoch :  0015 , Cost :  0.068182\n",
      "Epoch :  0016 , Cost :  0.118770\n",
      "Epoch :  0017 , Cost :  0.173052\n",
      "Epoch :  0018 , Cost :  0.022925\n",
      "Epoch :  0019 , Cost :  0.032580\n",
      "Epoch :  0020 , Cost :  0.013977\n",
      "Epoch :  0021 , Cost :  0.019264\n",
      "Epoch :  0022 , Cost :  0.019271\n",
      "Epoch :  0023 , Cost :  0.005876\n",
      "Epoch :  0024 , Cost :  0.003719\n",
      "Epoch :  0025 , Cost :  0.007681\n",
      "Epoch :  0026 , Cost :  0.005537\n",
      "Epoch :  0027 , Cost :  0.002244\n",
      "Epoch :  0028 , Cost :  0.004290\n",
      "Epoch :  0029 , Cost :  0.003081\n",
      "Epoch :  0030 , Cost :  0.003696\n",
      "Epoch :  0031 , Cost :  0.001498\n",
      "Epoch :  0032 , Cost :  0.002180\n",
      "Epoch :  0033 , Cost :  0.002892\n",
      "Epoch :  0034 , Cost :  0.001325\n",
      "Epoch :  0035 , Cost :  0.001051\n",
      "Epoch :  0036 , Cost :  0.000643\n",
      "Epoch :  0037 , Cost :  0.001150\n",
      "Epoch :  0038 , Cost :  0.000727\n",
      "Epoch :  0039 , Cost :  0.001673\n",
      "Epoch :  0040 , Cost :  0.000643\n",
      "Epoch :  0041 , Cost :  0.001284\n",
      "Epoch :  0042 , Cost :  0.000917\n",
      "Epoch :  0043 , Cost :  0.000994\n",
      "Epoch :  0044 , Cost :  0.000597\n",
      "Epoch :  0045 , Cost :  0.001488\n",
      "Epoch :  0046 , Cost :  0.000496\n",
      "Epoch :  0047 , Cost :  0.000828\n",
      "Epoch :  0048 , Cost :  0.000701\n",
      "Epoch :  0049 , Cost :  0.001294\n",
      "Epoch :  0050 , Cost :  0.001082\n",
      "Epoch :  0051 , Cost :  0.001224\n",
      "Epoch :  0052 , Cost :  0.000595\n",
      "Epoch :  0053 , Cost :  0.000437\n",
      "Epoch :  0054 , Cost :  0.000794\n",
      "Epoch :  0055 , Cost :  0.000331\n",
      "Epoch :  0056 , Cost :  0.000672\n",
      "Epoch :  0057 , Cost :  0.000441\n",
      "Epoch :  0058 , Cost :  0.000514\n",
      "Epoch :  0059 , Cost :  0.001100\n",
      "Epoch :  0060 , Cost :  0.000796\n",
      "Epoch :  0061 , Cost :  0.005274\n",
      "Epoch :  0062 , Cost :  0.000386\n",
      "Epoch :  0063 , Cost :  0.001048\n",
      "Epoch :  0064 , Cost :  0.000912\n",
      "Epoch :  0065 , Cost :  0.000299\n",
      "Epoch :  0066 , Cost :  0.000358\n",
      "Epoch :  0067 , Cost :  0.000596\n",
      "Epoch :  0068 , Cost :  0.000920\n",
      "Epoch :  0069 , Cost :  0.000752\n",
      "Epoch :  0070 , Cost :  0.000792\n",
      "Epoch :  0071 , Cost :  0.000637\n",
      "Epoch :  0072 , Cost :  0.000942\n",
      "Epoch :  0073 , Cost :  0.000264\n",
      "Epoch :  0074 , Cost :  0.000783\n",
      "Epoch :  0075 , Cost :  0.000230\n",
      "Epoch :  0076 , Cost :  0.000316\n",
      "Epoch :  0077 , Cost :  0.000760\n",
      "Epoch :  0078 , Cost :  0.000409\n",
      "Epoch :  0079 , Cost :  0.000841\n",
      "Epoch :  0080 , Cost :  0.000888\n",
      "Epoch :  0081 , Cost :  0.000490\n",
      "Epoch :  0082 , Cost :  0.000555\n",
      "Epoch :  0083 , Cost :  0.000428\n",
      "Epoch :  0084 , Cost :  0.000213\n",
      "Epoch :  0085 , Cost :  0.001977\n",
      "Epoch :  0086 , Cost :  0.000348\n",
      "Epoch :  0087 , Cost :  0.000304\n",
      "Epoch :  0088 , Cost :  0.000752\n",
      "Epoch :  0089 , Cost :  0.000327\n",
      "Epoch :  0090 , Cost :  0.000946\n",
      "Epoch :  0091 , Cost :  0.000553\n",
      "Epoch :  0092 , Cost :  0.000381\n",
      "Epoch :  0093 , Cost :  0.000203\n",
      "Epoch :  0094 , Cost :  0.000464\n",
      "Epoch :  0095 , Cost :  0.000277\n",
      "Epoch :  0096 , Cost :  0.000316\n",
      "Epoch :  0097 , Cost :  0.000231\n",
      "Epoch :  0098 , Cost :  0.000721\n",
      "Epoch :  0099 , Cost :  0.000611\n",
      "Epoch :  0100 , Cost :  0.001185\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "input_batch, output_batch, target_batch=make_batch(seq_data)\n",
    "for epoch in range(total_epoch):\n",
    "    _,loss = sess.run([optimizer, cost], feed_dict={enc_input:input_batch, dec_input:output_batch, targets:target_batch})\n",
    "    print('Epoch : ','%04d'%(epoch+1), ', Cost : ','{:.6f}'.format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(word):\n",
    "    seq_data = [word, 'P'*len(word)] #기존의 공백으로 output대신 무의미한 단어를 넣어준다.\n",
    "    input_batch, output_batch, target_gatch = make_batch([seq_data])\n",
    "    prediction = tf.argmax(model,2) #41개의 확률값으로 나온다.\n",
    "    result = sess.run(prediction, feed_dict={enc_input:input_batch, dec_input:output_batch, targets:target_batch})\n",
    "    decoded = [char_arr[i] for i in result[0]] #숫자예측된놈을 문자로 바꾼다.\n",
    "    end = decoded.index('E') #단어끝은 'E'\n",
    "    translated = ''.join(decoded[:end])\n",
    "    return translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word ->  단어\n",
      "wood ->  나무\n"
     ]
    }
   ],
   "source": [
    "print('word -> ', translate('word'))\n",
    "print('wood -> ', translate('wood'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#papago openAPI를 이용한 번역서비스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"message\":{\"@type\":\"response\",\"@service\":\"naverservice.nmt.proxy\",\"@version\":\"1.0.0\",\"result\":{\"srcLangType\":\"ko\",\"tarLangType\":\"en\",\"translatedText\":\"Let's try with a happy heart. Until the day you want it.\",\"engineType\":\"UNDEF_MULTI_SENTENCE\",\"pivot\":null}}}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import urllib.request\n",
    "client_id = 'EkFhkv4Yg0GGcvKTA2DF'\n",
    "client_secret = 'ajMm0rD5qo'\n",
    "encText = urllib.parse.quote('즐거운 마음으로 노력합시다. 원하는대로 되는 날까지')\n",
    "\n",
    "#rest API : 원격으로 함수호출 하듯이 api를 호출\n",
    "data = 'source=ko&target=en&text=' + encText\n",
    "url = 'https://openapi.naver.com/v1/papago/n2mt'\n",
    "request = urllib.request.Request(url) #마치 웹브라우저처럼 html로 데이터를 받는다. web은 데이터 교환포멧이 json으로 통일되어있다.\n",
    "request.add_header('X-Naver-Client-Id', client_id)\n",
    "request.add_header('X-Naver-Client-Secret', client_secret)\n",
    "response = urllib.request.urlopen(request, data=data.encode('utf-8'))\n",
    "rescode = response.getcode()\n",
    "if (rescode == 200):\n",
    "    response_body = response.read()\n",
    "    print(response_body.decode('utf-8'))\n",
    "else : \n",
    "    print('Error Code'+rescode)\n",
    "#json 형식으로 주고받아도 text로 결과가 전달된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'message': {'@type': 'response', '@service': 'naverservice.nmt.proxy', '@version': '1.0.0', 'result': {'srcLangType': 'ko', 'tarLangType': 'en', 'translatedText': \"Let's try with a happy heart. Until the day you want it.\", 'engineType': 'UNDEF_MULTI_SENTENCE', 'pivot': None}}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'message': {'@type': 'response',\n",
       "  '@service': 'naverservice.nmt.proxy',\n",
       "  '@version': '1.0.0',\n",
       "  'result': {'srcLangType': 'ko',\n",
       "   'tarLangType': 'en',\n",
       "   'translatedText': \"Let's try with a happy heart. Until the day you want it.\",\n",
       "   'engineType': 'UNDEF_MULTI_SENTENCE',\n",
       "   'pivot': None}}}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "print(type(data))\n",
    "data = json.loads(response_body)\n",
    "print(data)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Let's try with a happy heart. Until the day you want it.\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['message']['result']['translatedText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keras ( tensorflow에서도 keras로 모델을 만드는 것을 권장한다.)\n",
    "    #model(layers)\n",
    "    #compile(cost, optimizer, metrics)\n",
    "    #fit(batch_size, epochs, 훈련데이터) - 훈련\n",
    "    #evaluate(test데이터) - 평가\n",
    "    #predict - 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image, SVG\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras.datasets import boston_housing\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils.vis_utils import model_to_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/keras-datasets/boston_housing.npz\n",
      "57344/57026 [==============================] - 0s 6us/step\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404, 13)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102, 13)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13,)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_features = np.prod(x_train.shape[1:])\n",
    "num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 1)                 14        \n",
      "=================================================================\n",
      "Total params: 14\n",
      "Trainable params: 14\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential() #13개의 특징이 들어와서 1개로 출력된다.(가중치=13*1, 바이어스=1개 -> 14개를 가지고 있다.)\n",
    "#Param # - 가중치 사이즈, 가중치로 소비되어지는 메모리 사이즈를 얘기한다.\n",
    "model.add(Dense(1,input_dim=num_features, activation='linear')) #activation='linear 선형회귀\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sequential, functional(multi input, multi output), model(클래스)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"157pt\" viewBox=\"0.00 0.00 174.00 118.00\" width=\"232pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1.33333 1.33333) rotate(0) translate(4 114)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-114 170,-114 170,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 2738756405896 -->\n",
       "<g class=\"node\" id=\"node1\"><title>2738756405896</title>\n",
       "<polygon fill=\"none\" points=\"0,-73.5 0,-109.5 166,-109.5 166,-73.5 0,-73.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"83\" y=\"-87.8\">dense_2_input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 2738755906952 -->\n",
       "<g class=\"node\" id=\"node2\"><title>2738755906952</title>\n",
       "<polygon fill=\"none\" points=\"31,-0.5 31,-36.5 135,-36.5 135,-0.5 31,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"83\" y=\"-14.8\">dense_2: Dense</text>\n",
       "</g>\n",
       "<!-- 2738756405896&#45;&gt;2738755906952 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>2738756405896-&gt;2738755906952</title>\n",
       "<path d=\"M83,-73.3129C83,-65.2895 83,-55.5475 83,-46.5691\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"86.5001,-46.5288 83,-36.5288 79.5001,-46.5289 86.5001,-46.5288\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "404/404 [==============================] - 1s 2ms/step - loss: 1629.4415 - mae: 31.2132\n",
      "Epoch 2/10\n",
      "404/404 [==============================] - 1s 1ms/step - loss: 303.4441 - mae: 13.3414\n",
      "Epoch 3/10\n",
      "404/404 [==============================] - 1s 1ms/step - loss: 175.8556 - mae: 10.2532\n",
      "Epoch 4/10\n",
      "404/404 [==============================] - 1s 1ms/step - loss: 139.5528 - mae: 9.0091\n",
      "Epoch 5/10\n",
      "404/404 [==============================] - 1s 1ms/step - loss: 122.5425 - mae: 8.3356\n",
      "Epoch 6/10\n",
      "404/404 [==============================] - 1s 1ms/step - loss: 109.1184 - mae: 7.6820\n",
      "Epoch 7/10\n",
      "404/404 [==============================] - 1s 1ms/step - loss: 97.0942 - mae: 7.1625\n",
      "Epoch 8/10\n",
      "404/404 [==============================] - 1s 1ms/step - loss: 90.3998 - mae: 6.6476\n",
      "Epoch 9/10\n",
      "404/404 [==============================] - 1s 1ms/step - loss: 82.7242 - mae: 6.4822\n",
      "Epoch 10/10\n",
      "404/404 [==============================] - 1s 1ms/step - loss: 77.7603 - mae: 6.2154\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x27dabc4ec48>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#compile : loss, optimizer, metrics 를 지정\n",
    "model.compile(optimizer='rmsprop', loss='mse', metrics=['mae']) #mae를 이용해서 평가\n",
    "#compile을 해주여야한다. (fit()에서 이중for문의 역할을 한다.)\n",
    "#변수를 사용하지 않더라도 자동으로 호출되어서 진행된다.\n",
    "model.fit(x_train, y_train, batch_size=1, epochs=10, verbose=1) #verbose=1 - 진행상황을 보여줘라 , verbose=False 결과만 보여줘라"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(81.28032160740273, 9.015559971926466, 6.251389980316162)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse, mae = model.evaluate(x_test, y_test, verbose=False)\n",
    "rmse = np.sqrt(mse)\n",
    "mse, rmse, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.1159942],\n",
       "       [19.294737 ],\n",
       "       [21.83674  ]], dtype=float32)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model.predict(x_test[:3,:])\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7.2, 18.8, 19. ])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real = y_test[:3]\n",
    "real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.9</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3  4\n",
       "1  6.4  2.8  5.6  2.2  2\n",
       "2  5.0  2.3  3.3    1  1\n",
       "3  4.9  2.5  4.5  1.7  2\n",
       "4  4.9  3.1  1.5  0.1  0\n",
       "5  5.7  3.8  1.7  0.3  0"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_csv('iris.csv', header=None)\n",
    "dataset.head()\n",
    "dataset = dataset.iloc[1:,:]\n",
    "print(dataset.shape)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(150, 3)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "X = dataset.iloc[:,:4] #독립변수\n",
    "Y = dataset.iloc[:,4] #종속변수\n",
    "Y = np.asarray(Y)\n",
    "Y = keras.utils.to_categorical(Y) #one hot encoding\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential #input, utput 1개씩 존재\n",
    "from keras.wrappers.scikit_learn import KerasClassifier #keras의 모델과 scikit의 모델의 호환성을 가진다.\n",
    "from keras.layers import Dense #dense = ffnn망 fully connected망\n",
    "from sklearn.model_selection import GridSearchCV #parameter를 grid를 만들고 best param을 찾는다.\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 105 samples, validate on 45 samples\n",
      "Epoch 1/100\n",
      "105/105 [==============================] - 0s 3ms/step - loss: 1.9257 - accuracy: 0.3810 - val_loss: 2.3821 - val_accuracy: 0.2222\n",
      "Epoch 2/100\n",
      "105/105 [==============================] - 0s 272us/step - loss: 1.7294 - accuracy: 0.4095 - val_loss: 2.1207 - val_accuracy: 0.5333\n",
      "Epoch 3/100\n",
      "105/105 [==============================] - 0s 266us/step - loss: 1.5515 - accuracy: 0.6762 - val_loss: 1.8655 - val_accuracy: 0.5556\n",
      "Epoch 4/100\n",
      "105/105 [==============================] - 0s 256us/step - loss: 1.3678 - accuracy: 0.7048 - val_loss: 1.6280 - val_accuracy: 0.5778\n",
      "Epoch 5/100\n",
      "105/105 [==============================] - 0s 256us/step - loss: 1.1931 - accuracy: 0.7048 - val_loss: 1.3973 - val_accuracy: 0.5778\n",
      "Epoch 6/100\n",
      "105/105 [==============================] - 0s 254us/step - loss: 1.0344 - accuracy: 0.7048 - val_loss: 1.1821 - val_accuracy: 0.5778\n",
      "Epoch 7/100\n",
      "105/105 [==============================] - 0s 277us/step - loss: 0.8957 - accuracy: 0.7048 - val_loss: 1.0072 - val_accuracy: 0.5778\n",
      "Epoch 8/100\n",
      "105/105 [==============================] - 0s 285us/step - loss: 0.8027 - accuracy: 0.7048 - val_loss: 0.8707 - val_accuracy: 0.5778\n",
      "Epoch 9/100\n",
      "105/105 [==============================] - 0s 294us/step - loss: 0.7401 - accuracy: 0.7048 - val_loss: 0.7890 - val_accuracy: 0.5778\n",
      "Epoch 10/100\n",
      "105/105 [==============================] - 0s 298us/step - loss: 0.7047 - accuracy: 0.7048 - val_loss: 0.7405 - val_accuracy: 0.6444\n",
      "Epoch 11/100\n",
      "105/105 [==============================] - 0s 285us/step - loss: 0.6722 - accuracy: 0.7238 - val_loss: 0.7124 - val_accuracy: 0.6667\n",
      "Epoch 12/100\n",
      "105/105 [==============================] - 0s 266us/step - loss: 0.6462 - accuracy: 0.7143 - val_loss: 0.6951 - val_accuracy: 0.6444\n",
      "Epoch 13/100\n",
      "105/105 [==============================] - 0s 248us/step - loss: 0.6248 - accuracy: 0.7143 - val_loss: 0.6764 - val_accuracy: 0.6000\n",
      "Epoch 14/100\n",
      "105/105 [==============================] - 0s 288us/step - loss: 0.6045 - accuracy: 0.7143 - val_loss: 0.6474 - val_accuracy: 0.6444\n",
      "Epoch 15/100\n",
      "105/105 [==============================] - 0s 256us/step - loss: 0.5846 - accuracy: 0.7143 - val_loss: 0.6354 - val_accuracy: 0.6222\n",
      "Epoch 16/100\n",
      "105/105 [==============================] - 0s 283us/step - loss: 0.5664 - accuracy: 0.7238 - val_loss: 0.6150 - val_accuracy: 0.6444\n",
      "Epoch 17/100\n",
      "105/105 [==============================] - 0s 266us/step - loss: 0.5528 - accuracy: 0.7429 - val_loss: 0.5839 - val_accuracy: 0.7333\n",
      "Epoch 18/100\n",
      "105/105 [==============================] - 0s 247us/step - loss: 0.5360 - accuracy: 0.7619 - val_loss: 0.5720 - val_accuracy: 0.7333\n",
      "Epoch 19/100\n",
      "105/105 [==============================] - 0s 300us/step - loss: 0.5233 - accuracy: 0.7429 - val_loss: 0.5625 - val_accuracy: 0.6889\n",
      "Epoch 20/100\n",
      "105/105 [==============================] - 0s 244us/step - loss: 0.5101 - accuracy: 0.7524 - val_loss: 0.5464 - val_accuracy: 0.7556\n",
      "Epoch 21/100\n",
      "105/105 [==============================] - 0s 294us/step - loss: 0.4994 - accuracy: 0.7619 - val_loss: 0.5385 - val_accuracy: 0.7111\n",
      "Epoch 22/100\n",
      "105/105 [==============================] - 0s 257us/step - loss: 0.4919 - accuracy: 0.7429 - val_loss: 0.5341 - val_accuracy: 0.6889\n",
      "Epoch 23/100\n",
      "105/105 [==============================] - 0s 433us/step - loss: 0.4800 - accuracy: 0.7905 - val_loss: 0.5029 - val_accuracy: 0.8444\n",
      "Epoch 24/100\n",
      "105/105 [==============================] - 0s 475us/step - loss: 0.4718 - accuracy: 0.8571 - val_loss: 0.4926 - val_accuracy: 0.8667\n",
      "Epoch 25/100\n",
      "105/105 [==============================] - 0s 491us/step - loss: 0.4611 - accuracy: 0.8381 - val_loss: 0.4982 - val_accuracy: 0.7778\n",
      "Epoch 26/100\n",
      "105/105 [==============================] - 0s 508us/step - loss: 0.4536 - accuracy: 0.7714 - val_loss: 0.4933 - val_accuracy: 0.7778\n",
      "Epoch 27/100\n",
      "105/105 [==============================] - 0s 498us/step - loss: 0.4494 - accuracy: 0.8286 - val_loss: 0.4752 - val_accuracy: 0.8444\n",
      "Epoch 28/100\n",
      "105/105 [==============================] - 0s 465us/step - loss: 0.4397 - accuracy: 0.8476 - val_loss: 0.4719 - val_accuracy: 0.8222\n",
      "Epoch 29/100\n",
      "105/105 [==============================] - 0s 485us/step - loss: 0.4347 - accuracy: 0.8095 - val_loss: 0.4759 - val_accuracy: 0.8222\n",
      "Epoch 30/100\n",
      "105/105 [==============================] - 0s 532us/step - loss: 0.4287 - accuracy: 0.8190 - val_loss: 0.4559 - val_accuracy: 0.8444\n",
      "Epoch 31/100\n",
      "105/105 [==============================] - 0s 490us/step - loss: 0.4227 - accuracy: 0.8762 - val_loss: 0.4418 - val_accuracy: 0.9111\n",
      "Epoch 32/100\n",
      "105/105 [==============================] - 0s 485us/step - loss: 0.4163 - accuracy: 0.8857 - val_loss: 0.4378 - val_accuracy: 0.8889\n",
      "Epoch 33/100\n",
      "105/105 [==============================] - 0s 503us/step - loss: 0.4125 - accuracy: 0.8762 - val_loss: 0.4401 - val_accuracy: 0.8444\n",
      "Epoch 34/100\n",
      "105/105 [==============================] - 0s 573us/step - loss: 0.4065 - accuracy: 0.8762 - val_loss: 0.4273 - val_accuracy: 0.8889\n",
      "Epoch 35/100\n",
      "105/105 [==============================] - 0s 584us/step - loss: 0.4018 - accuracy: 0.9048 - val_loss: 0.4191 - val_accuracy: 0.9111\n",
      "Epoch 36/100\n",
      "105/105 [==============================] - 0s 423us/step - loss: 0.3962 - accuracy: 0.8952 - val_loss: 0.4171 - val_accuracy: 0.8889\n",
      "Epoch 37/100\n",
      "105/105 [==============================] - 0s 275us/step - loss: 0.3930 - accuracy: 0.8762 - val_loss: 0.4195 - val_accuracy: 0.8667\n",
      "Epoch 38/100\n",
      "105/105 [==============================] - 0s 260us/step - loss: 0.3869 - accuracy: 0.8762 - val_loss: 0.4082 - val_accuracy: 0.8889\n",
      "Epoch 39/100\n",
      "105/105 [==============================] - 0s 233us/step - loss: 0.3853 - accuracy: 0.9238 - val_loss: 0.3900 - val_accuracy: 0.9778\n",
      "Epoch 40/100\n",
      "105/105 [==============================] - 0s 230us/step - loss: 0.3791 - accuracy: 0.9429 - val_loss: 0.3948 - val_accuracy: 0.9111\n",
      "Epoch 41/100\n",
      "105/105 [==============================] - 0s 247us/step - loss: 0.3755 - accuracy: 0.8857 - val_loss: 0.4023 - val_accuracy: 0.8667\n",
      "Epoch 42/100\n",
      "105/105 [==============================] - 0s 285us/step - loss: 0.3710 - accuracy: 0.8857 - val_loss: 0.3913 - val_accuracy: 0.8889\n",
      "Epoch 43/100\n",
      "105/105 [==============================] - 0s 332us/step - loss: 0.3678 - accuracy: 0.8857 - val_loss: 0.3904 - val_accuracy: 0.8889\n",
      "Epoch 44/100\n",
      "105/105 [==============================] - 0s 295us/step - loss: 0.3630 - accuracy: 0.8952 - val_loss: 0.3800 - val_accuracy: 0.9111\n",
      "Epoch 45/100\n",
      "105/105 [==============================] - 0s 294us/step - loss: 0.3592 - accuracy: 0.9238 - val_loss: 0.3722 - val_accuracy: 0.9333\n",
      "Epoch 46/100\n",
      "105/105 [==============================] - 0s 304us/step - loss: 0.3547 - accuracy: 0.9524 - val_loss: 0.3662 - val_accuracy: 0.9111\n",
      "Epoch 47/100\n",
      "105/105 [==============================] - 0s 314us/step - loss: 0.3531 - accuracy: 0.9524 - val_loss: 0.3575 - val_accuracy: 0.9778\n",
      "Epoch 48/100\n",
      "105/105 [==============================] - 0s 285us/step - loss: 0.3463 - accuracy: 0.9524 - val_loss: 0.3616 - val_accuracy: 0.9111\n",
      "Epoch 49/100\n",
      "105/105 [==============================] - 0s 275us/step - loss: 0.3445 - accuracy: 0.8952 - val_loss: 0.3629 - val_accuracy: 0.9111\n",
      "Epoch 50/100\n",
      "105/105 [==============================] - 0s 296us/step - loss: 0.3393 - accuracy: 0.9143 - val_loss: 0.3541 - val_accuracy: 0.9111\n",
      "Epoch 51/100\n",
      "105/105 [==============================] - 0s 266us/step - loss: 0.3386 - accuracy: 0.9524 - val_loss: 0.3371 - val_accuracy: 0.9778\n",
      "Epoch 52/100\n",
      "105/105 [==============================] - 0s 275us/step - loss: 0.3328 - accuracy: 0.9524 - val_loss: 0.3386 - val_accuracy: 0.9778\n",
      "Epoch 53/100\n",
      "105/105 [==============================] - 0s 275us/step - loss: 0.3293 - accuracy: 0.9429 - val_loss: 0.3468 - val_accuracy: 0.9111\n",
      "Epoch 54/100\n",
      "105/105 [==============================] - 0s 274us/step - loss: 0.3276 - accuracy: 0.8857 - val_loss: 0.3428 - val_accuracy: 0.9111\n",
      "Epoch 55/100\n",
      "105/105 [==============================] - 0s 276us/step - loss: 0.3226 - accuracy: 0.9238 - val_loss: 0.3329 - val_accuracy: 0.9556\n",
      "Epoch 56/100\n",
      "105/105 [==============================] - 0s 268us/step - loss: 0.3191 - accuracy: 0.9524 - val_loss: 0.3275 - val_accuracy: 0.9778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100\n",
      "105/105 [==============================] - 0s 287us/step - loss: 0.3158 - accuracy: 0.9524 - val_loss: 0.3227 - val_accuracy: 0.9778\n",
      "Epoch 58/100\n",
      "105/105 [==============================] - 0s 257us/step - loss: 0.3117 - accuracy: 0.9524 - val_loss: 0.3125 - val_accuracy: 0.9778\n",
      "Epoch 59/100\n",
      "105/105 [==============================] - 0s 268us/step - loss: 0.3095 - accuracy: 0.9524 - val_loss: 0.3086 - val_accuracy: 0.9778\n",
      "Epoch 60/100\n",
      "105/105 [==============================] - 0s 273us/step - loss: 0.3064 - accuracy: 0.9524 - val_loss: 0.3076 - val_accuracy: 0.9778\n",
      "Epoch 61/100\n",
      "105/105 [==============================] - 0s 270us/step - loss: 0.3054 - accuracy: 0.9333 - val_loss: 0.3197 - val_accuracy: 0.9333\n",
      "Epoch 62/100\n",
      "105/105 [==============================] - 0s 267us/step - loss: 0.3037 - accuracy: 0.9429 - val_loss: 0.2979 - val_accuracy: 0.9778\n",
      "Epoch 63/100\n",
      "105/105 [==============================] - 0s 427us/step - loss: 0.2971 - accuracy: 0.9524 - val_loss: 0.3011 - val_accuracy: 0.9778\n",
      "Epoch 64/100\n",
      "105/105 [==============================] - 0s 449us/step - loss: 0.2948 - accuracy: 0.9524 - val_loss: 0.2955 - val_accuracy: 0.9778\n",
      "Epoch 65/100\n",
      "105/105 [==============================] - 0s 427us/step - loss: 0.2913 - accuracy: 0.9524 - val_loss: 0.2935 - val_accuracy: 0.9778\n",
      "Epoch 66/100\n",
      "105/105 [==============================] - 0s 456us/step - loss: 0.2882 - accuracy: 0.9524 - val_loss: 0.2889 - val_accuracy: 0.9778\n",
      "Epoch 67/100\n",
      "105/105 [==============================] - 0s 438us/step - loss: 0.2864 - accuracy: 0.9524 - val_loss: 0.2907 - val_accuracy: 0.9778\n",
      "Epoch 68/100\n",
      "105/105 [==============================] - 0s 441us/step - loss: 0.2831 - accuracy: 0.9524 - val_loss: 0.2806 - val_accuracy: 0.9778\n",
      "Epoch 69/100\n",
      "105/105 [==============================] - 0s 456us/step - loss: 0.2806 - accuracy: 0.9619 - val_loss: 0.2833 - val_accuracy: 0.9778\n",
      "Epoch 70/100\n",
      "105/105 [==============================] - 0s 475us/step - loss: 0.2775 - accuracy: 0.9524 - val_loss: 0.2789 - val_accuracy: 0.9778\n",
      "Epoch 71/100\n",
      "105/105 [==============================] - 0s 433us/step - loss: 0.2741 - accuracy: 0.9619 - val_loss: 0.2703 - val_accuracy: 0.9778\n",
      "Epoch 72/100\n",
      "105/105 [==============================] - 0s 475us/step - loss: 0.2722 - accuracy: 0.9619 - val_loss: 0.2700 - val_accuracy: 0.9778\n",
      "Epoch 73/100\n",
      "105/105 [==============================] - 0s 541us/step - loss: 0.2702 - accuracy: 0.9524 - val_loss: 0.2668 - val_accuracy: 0.9778\n",
      "Epoch 74/100\n",
      "105/105 [==============================] - 0s 530us/step - loss: 0.2677 - accuracy: 0.9619 - val_loss: 0.2652 - val_accuracy: 0.9778\n",
      "Epoch 75/100\n",
      "105/105 [==============================] - 0s 513us/step - loss: 0.2638 - accuracy: 0.9619 - val_loss: 0.2577 - val_accuracy: 0.9778\n",
      "Epoch 76/100\n",
      "105/105 [==============================] - 0s 481us/step - loss: 0.2625 - accuracy: 0.9714 - val_loss: 0.2555 - val_accuracy: 0.9778\n",
      "Epoch 77/100\n",
      "105/105 [==============================] - 0s 481us/step - loss: 0.2597 - accuracy: 0.9714 - val_loss: 0.2557 - val_accuracy: 0.9778\n",
      "Epoch 78/100\n",
      "105/105 [==============================] - 0s 456us/step - loss: 0.2582 - accuracy: 0.9619 - val_loss: 0.2499 - val_accuracy: 0.9778\n",
      "Epoch 79/100\n",
      "105/105 [==============================] - 0s 465us/step - loss: 0.2542 - accuracy: 0.9619 - val_loss: 0.2537 - val_accuracy: 0.9778\n",
      "Epoch 80/100\n",
      "105/105 [==============================] - 0s 475us/step - loss: 0.2527 - accuracy: 0.9619 - val_loss: 0.2577 - val_accuracy: 0.9778\n",
      "Epoch 81/100\n",
      "105/105 [==============================] - 0s 465us/step - loss: 0.2513 - accuracy: 0.9524 - val_loss: 0.2505 - val_accuracy: 0.9778\n",
      "Epoch 82/100\n",
      "105/105 [==============================] - 0s 481us/step - loss: 0.2485 - accuracy: 0.9619 - val_loss: 0.2386 - val_accuracy: 0.9778\n",
      "Epoch 83/100\n",
      "105/105 [==============================] - 0s 446us/step - loss: 0.2464 - accuracy: 0.9810 - val_loss: 0.2350 - val_accuracy: 0.9778\n",
      "Epoch 84/100\n",
      "105/105 [==============================] - 0s 446us/step - loss: 0.2435 - accuracy: 0.9619 - val_loss: 0.2411 - val_accuracy: 0.9778\n",
      "Epoch 85/100\n",
      "105/105 [==============================] - 0s 475us/step - loss: 0.2448 - accuracy: 0.9524 - val_loss: 0.2437 - val_accuracy: 0.9778\n",
      "Epoch 86/100\n",
      "105/105 [==============================] - 0s 488us/step - loss: 0.2460 - accuracy: 0.9619 - val_loss: 0.2219 - val_accuracy: 0.9778\n",
      "Epoch 87/100\n",
      "105/105 [==============================] - 0s 455us/step - loss: 0.2391 - accuracy: 0.9714 - val_loss: 0.2290 - val_accuracy: 0.9778\n",
      "Epoch 88/100\n",
      "105/105 [==============================] - 0s 471us/step - loss: 0.2347 - accuracy: 0.9714 - val_loss: 0.2249 - val_accuracy: 0.9778\n",
      "Epoch 89/100\n",
      "105/105 [==============================] - 0s 465us/step - loss: 0.2331 - accuracy: 0.9619 - val_loss: 0.2274 - val_accuracy: 0.9778\n",
      "Epoch 90/100\n",
      "105/105 [==============================] - 0s 556us/step - loss: 0.2305 - accuracy: 0.9619 - val_loss: 0.2229 - val_accuracy: 0.9778\n",
      "Epoch 91/100\n",
      "105/105 [==============================] - 0s 522us/step - loss: 0.2285 - accuracy: 0.9714 - val_loss: 0.2169 - val_accuracy: 0.9778\n",
      "Epoch 92/100\n",
      "105/105 [==============================] - 0s 531us/step - loss: 0.2274 - accuracy: 0.9810 - val_loss: 0.2113 - val_accuracy: 0.9778\n",
      "Epoch 93/100\n",
      "105/105 [==============================] - 0s 495us/step - loss: 0.2268 - accuracy: 0.9619 - val_loss: 0.2178 - val_accuracy: 0.9778\n",
      "Epoch 94/100\n",
      "105/105 [==============================] - 0s 480us/step - loss: 0.2234 - accuracy: 0.9619 - val_loss: 0.2145 - val_accuracy: 0.9778\n",
      "Epoch 95/100\n",
      "105/105 [==============================] - 0s 448us/step - loss: 0.2231 - accuracy: 0.9810 - val_loss: 0.2045 - val_accuracy: 0.9778\n",
      "Epoch 96/100\n",
      "105/105 [==============================] - 0s 443us/step - loss: 0.2193 - accuracy: 0.9714 - val_loss: 0.2131 - val_accuracy: 0.9778\n",
      "Epoch 97/100\n",
      "105/105 [==============================] - 0s 451us/step - loss: 0.2173 - accuracy: 0.9619 - val_loss: 0.2086 - val_accuracy: 0.9778\n",
      "Epoch 98/100\n",
      "105/105 [==============================] - 0s 471us/step - loss: 0.2160 - accuracy: 0.9714 - val_loss: 0.2022 - val_accuracy: 0.9778\n",
      "Epoch 99/100\n",
      "105/105 [==============================] - 0s 484us/step - loss: 0.2160 - accuracy: 0.9619 - val_loss: 0.2109 - val_accuracy: 0.9778\n",
      "Epoch 100/100\n",
      "105/105 [==============================] - 0s 460us/step - loss: 0.2149 - accuracy: 0.9714 - val_loss: 0.1952 - val_accuracy: 0.9778\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x27dac6efa88>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential() #입력은 자동으로 된다. (이전레이어의 출력값이 다음레이어의 입력값으로 들어간다.)\n",
    "model.add(Dense(15, input_dim=4, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "#categorical_crossentropy - 분류문제를 다루는 크로스 엔트로피 , 확률을 불순도로 변경해서 최적화의 대상(cost function)이 되도록한다.\n",
    "#adam \n",
    "    #adagrad(처음에는 크게 점점작게, 0이되는것을 방지하기 위해 ada delta등장) -> RMSProp(상태를 보면서 러닝레이트를 줄인다.)\n",
    "    # + momentum\n",
    "#metrics 는 여러값을 넣을수 있으니 list자료형으ㅊ로 받는다.\n",
    "model.fit(X,Y,epochs=100, batch_size=10, validation_split=0.3) #7:3으로 데이터를 나눠라 -> train / validation(과적합을 확인하기 위해)\n",
    "#X의 차수 150*4 , 가중치 사이스 4*15, 나가는 데이터 150*15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model():\n",
    "    #model을 하나의 함수에 담았다.\n",
    "    model = Sequential()\n",
    "    model.add(Dense(8, input_dim=4, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 1.0990 - accuracy: 0.2667\n",
      "Epoch 2/100\n",
      "150/150 [==============================] - 0s 199us/step - loss: 1.0987 - accuracy: 0.2667\n",
      "Epoch 3/100\n",
      "150/150 [==============================] - 0s 186us/step - loss: 1.0987 - accuracy: 0.3067\n",
      "Epoch 4/100\n",
      "150/150 [==============================] - 0s 185us/step - loss: 1.0987 - accuracy: 0.3000\n",
      "Epoch 5/100\n",
      "150/150 [==============================] - 0s 186us/step - loss: 1.0987 - accuracy: 0.3133\n",
      "Epoch 6/100\n",
      "150/150 [==============================] - 0s 192us/step - loss: 1.0987 - accuracy: 0.3000\n",
      "Epoch 7/100\n",
      "150/150 [==============================] - 0s 186us/step - loss: 1.0987 - accuracy: 0.3000\n",
      "Epoch 8/100\n",
      "150/150 [==============================] - 0s 193us/step - loss: 1.0987 - accuracy: 0.3133\n",
      "Epoch 9/100\n",
      "150/150 [==============================] - 0s 187us/step - loss: 1.0987 - accuracy: 0.3067\n",
      "Epoch 10/100\n",
      "150/150 [==============================] - 0s 173us/step - loss: 1.0986 - accuracy: 0.3333\n",
      "Epoch 11/100\n",
      "150/150 [==============================] - 0s 178us/step - loss: 1.0987 - accuracy: 0.2667\n",
      "Epoch 12/100\n",
      "150/150 [==============================] - 0s 199us/step - loss: 1.0986 - accuracy: 0.2733\n",
      "Epoch 13/100\n",
      "150/150 [==============================] - 0s 247us/step - loss: 1.0986 - accuracy: 0.2800\n",
      "Epoch 14/100\n",
      "150/150 [==============================] - 0s 206us/step - loss: 1.0987 - accuracy: 0.3333\n",
      "Epoch 15/100\n",
      "150/150 [==============================] - 0s 217us/step - loss: 1.0989 - accuracy: 0.3333\n",
      "Epoch 16/100\n",
      "150/150 [==============================] - 0s 219us/step - loss: 1.0987 - accuracy: 0.3333\n",
      "Epoch 17/100\n",
      "150/150 [==============================] - 0s 213us/step - loss: 1.0987 - accuracy: 0.3067\n",
      "Epoch 18/100\n",
      "150/150 [==============================] - 0s 222us/step - loss: 1.0989 - accuracy: 0.2667\n",
      "Epoch 19/100\n",
      "150/150 [==============================] - 0s 226us/step - loss: 1.0987 - accuracy: 0.3267\n",
      "Epoch 20/100\n",
      "150/150 [==============================] - 0s 320us/step - loss: 1.0987 - accuracy: 0.3333\n",
      "Epoch 21/100\n",
      "150/150 [==============================] - 0s 329us/step - loss: 1.0987 - accuracy: 0.2600\n",
      "Epoch 22/100\n",
      "150/150 [==============================] - 0s 366us/step - loss: 1.0988 - accuracy: 0.2933\n",
      "Epoch 23/100\n",
      "150/150 [==============================] - 0s 426us/step - loss: 1.0987 - accuracy: 0.3133\n",
      "Epoch 24/100\n",
      "150/150 [==============================] - 0s 360us/step - loss: 1.0987 - accuracy: 0.2800\n",
      "Epoch 25/100\n",
      "150/150 [==============================] - 0s 339us/step - loss: 1.0987 - accuracy: 0.2733\n",
      "Epoch 26/100\n",
      "150/150 [==============================] - 0s 347us/step - loss: 1.0987 - accuracy: 0.3200\n",
      "Epoch 27/100\n",
      "150/150 [==============================] - 0s 425us/step - loss: 1.0987 - accuracy: 0.2533\n",
      "Epoch 28/100\n",
      "150/150 [==============================] - 0s 445us/step - loss: 1.0988 - accuracy: 0.3333\n",
      "Epoch 29/100\n",
      "150/150 [==============================] - 0s 375us/step - loss: 1.0987 - accuracy: 0.3333\n",
      "Epoch 30/100\n",
      "150/150 [==============================] - 0s 332us/step - loss: 1.0987 - accuracy: 0.2933\n",
      "Epoch 31/100\n",
      "150/150 [==============================] - 0s 346us/step - loss: 1.0987 - accuracy: 0.3333\n",
      "Epoch 32/100\n",
      "150/150 [==============================] - 0s 372us/step - loss: 1.0987 - accuracy: 0.3333\n",
      "Epoch 33/100\n",
      "150/150 [==============================] - 0s 359us/step - loss: 1.0986 - accuracy: 0.2867\n",
      "Epoch 34/100\n",
      "150/150 [==============================] - 0s 324us/step - loss: 1.0987 - accuracy: 0.3333\n",
      "Epoch 35/100\n",
      "150/150 [==============================] - 0s 306us/step - loss: 1.0987 - accuracy: 0.2733\n",
      "Epoch 36/100\n",
      "150/150 [==============================] - 0s 518us/step - loss: 1.0987 - accuracy: 0.3333\n",
      "Epoch 37/100\n",
      "150/150 [==============================] - 0s 399us/step - loss: 1.0987 - accuracy: 0.3333\n",
      "Epoch 38/100\n",
      "150/150 [==============================] - 0s 166us/step - loss: 1.0987 - accuracy: 0.3333\n",
      "Epoch 39/100\n",
      "150/150 [==============================] - 0s 153us/step - loss: 1.0987 - accuracy: 0.3200\n",
      "Epoch 40/100\n",
      "150/150 [==============================] - 0s 141us/step - loss: 1.0987 - accuracy: 0.2933\n",
      "Epoch 41/100\n",
      "150/150 [==============================] - 0s 173us/step - loss: 1.0988 - accuracy: 0.2800\n",
      "Epoch 42/100\n",
      "150/150 [==============================] - 0s 193us/step - loss: 1.0988 - accuracy: 0.3333\n",
      "Epoch 43/100\n",
      "150/150 [==============================] - 0s 185us/step - loss: 1.0987 - accuracy: 0.2800\n",
      "Epoch 44/100\n",
      "150/150 [==============================] - 0s 193us/step - loss: 1.0987 - accuracy: 0.3333\n",
      "Epoch 45/100\n",
      "150/150 [==============================] - 0s 194us/step - loss: 1.0989 - accuracy: 0.2400\n",
      "Epoch 46/100\n",
      "150/150 [==============================] - 0s 195us/step - loss: 1.0988 - accuracy: 0.2867\n",
      "Epoch 47/100\n",
      "150/150 [==============================] - 0s 180us/step - loss: 1.0986 - accuracy: 0.3000\n",
      "Epoch 48/100\n",
      "150/150 [==============================] - 0s 226us/step - loss: 1.0987 - accuracy: 0.3067\n",
      "Epoch 49/100\n",
      "150/150 [==============================] - 0s 323us/step - loss: 1.0987 - accuracy: 0.3333\n",
      "Epoch 50/100\n",
      "150/150 [==============================] - 0s 293us/step - loss: 1.0987 - accuracy: 0.3333\n",
      "Epoch 51/100\n",
      "150/150 [==============================] - 0s 296us/step - loss: 1.0987 - accuracy: 0.2867\n",
      "Epoch 52/100\n",
      "150/150 [==============================] - 0s 193us/step - loss: 1.0987 - accuracy: 0.3333\n",
      "Epoch 53/100\n",
      "150/150 [==============================] - 0s 160us/step - loss: 1.0987 - accuracy: 0.2467\n",
      "Epoch 54/100\n",
      "150/150 [==============================] - 0s 267us/step - loss: 1.0987 - accuracy: 0.3000\n",
      "Epoch 55/100\n",
      "150/150 [==============================] - 0s 352us/step - loss: 1.0986 - accuracy: 0.3333\n",
      "Epoch 56/100\n",
      "150/150 [==============================] - 0s 329us/step - loss: 1.0987 - accuracy: 0.2867\n",
      "Epoch 57/100\n",
      "150/150 [==============================] - 0s 306us/step - loss: 1.0987 - accuracy: 0.2733\n",
      "Epoch 58/100\n",
      "150/150 [==============================] - 0s 344us/step - loss: 1.0989 - accuracy: 0.2600\n",
      "Epoch 59/100\n",
      "150/150 [==============================] - 0s 326us/step - loss: 1.0988 - accuracy: 0.3333\n",
      "Epoch 60/100\n",
      "150/150 [==============================] - 0s 312us/step - loss: 1.0986 - accuracy: 0.3333\n",
      "Epoch 61/100\n",
      "150/150 [==============================] - 0s 314us/step - loss: 1.0987 - accuracy: 0.3333\n",
      "Epoch 62/100\n",
      "150/150 [==============================] - 0s 312us/step - loss: 1.0987 - accuracy: 0.3333\n",
      "Epoch 63/100\n",
      "150/150 [==============================] - 0s 311us/step - loss: 1.0987 - accuracy: 0.2867\n",
      "Epoch 64/100\n",
      "150/150 [==============================] - 0s 311us/step - loss: 1.0987 - accuracy: 0.3200\n",
      "Epoch 65/100\n",
      "150/150 [==============================] - 0s 313us/step - loss: 1.0987 - accuracy: 0.2933\n",
      "Epoch 66/100\n",
      "150/150 [==============================] - 0s 313us/step - loss: 1.0988 - accuracy: 0.3333\n",
      "Epoch 67/100\n",
      "150/150 [==============================] - 0s 313us/step - loss: 1.0987 - accuracy: 0.3333\n",
      "Epoch 68/100\n",
      "150/150 [==============================] - 0s 312us/step - loss: 1.0988 - accuracy: 0.3133\n",
      "Epoch 69/100\n",
      "150/150 [==============================] - 0s 321us/step - loss: 1.0987 - accuracy: 0.2733\n",
      "Epoch 70/100\n",
      "150/150 [==============================] - 0s 314us/step - loss: 1.0987 - accuracy: 0.3333\n",
      "Epoch 71/100\n",
      "150/150 [==============================] - 0s 313us/step - loss: 1.0987 - accuracy: 0.3333\n",
      "Epoch 72/100\n",
      "150/150 [==============================] - 0s 316us/step - loss: 1.0987 - accuracy: 0.2933\n",
      "Epoch 73/100\n",
      "150/150 [==============================] - 0s 313us/step - loss: 1.0987 - accuracy: 0.3333\n",
      "Epoch 74/100\n",
      "150/150 [==============================] - 0s 320us/step - loss: 1.0987 - accuracy: 0.2933\n",
      "Epoch 75/100\n",
      "150/150 [==============================] - 0s 305us/step - loss: 1.0987 - accuracy: 0.2400\n",
      "Epoch 76/100\n",
      "150/150 [==============================] - 0s 313us/step - loss: 1.0987 - accuracy: 0.2867\n",
      "Epoch 77/100\n",
      "150/150 [==============================] - 0s 326us/step - loss: 1.0987 - accuracy: 0.3333\n",
      "Epoch 78/100\n",
      "150/150 [==============================] - 0s 308us/step - loss: 1.0987 - accuracy: 0.2867\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - 0s 306us/step - loss: 1.0987 - accuracy: 0.2933\n",
      "Epoch 80/100\n",
      "150/150 [==============================] - 0s 305us/step - loss: 1.0987 - accuracy: 0.2867\n",
      "Epoch 81/100\n",
      "150/150 [==============================] - 0s 306us/step - loss: 1.0987 - accuracy: 0.2800\n",
      "Epoch 82/100\n",
      "150/150 [==============================] - 0s 306us/step - loss: 1.0986 - accuracy: 0.3333\n",
      "Epoch 83/100\n",
      "150/150 [==============================] - 0s 239us/step - loss: 1.0987 - accuracy: 0.2733\n",
      "Epoch 84/100\n",
      "150/150 [==============================] - 0s 173us/step - loss: 1.0986 - accuracy: 0.3333\n",
      "Epoch 85/100\n",
      "150/150 [==============================] - 0s 172us/step - loss: 1.0987 - accuracy: 0.2600\n",
      "Epoch 86/100\n",
      "150/150 [==============================] - 0s 173us/step - loss: 1.0986 - accuracy: 0.3333\n",
      "Epoch 87/100\n",
      "150/150 [==============================] - 0s 181us/step - loss: 1.0987 - accuracy: 0.3000\n",
      "Epoch 88/100\n",
      "150/150 [==============================] - 0s 174us/step - loss: 1.0987 - accuracy: 0.3333\n",
      "Epoch 89/100\n",
      "150/150 [==============================] - 0s 173us/step - loss: 1.0987 - accuracy: 0.3000\n",
      "Epoch 90/100\n",
      "150/150 [==============================] - 0s 184us/step - loss: 1.0987 - accuracy: 0.3333\n",
      "Epoch 91/100\n",
      "150/150 [==============================] - 0s 180us/step - loss: 1.0987 - accuracy: 0.3333\n",
      "Epoch 92/100\n",
      "150/150 [==============================] - 0s 173us/step - loss: 1.0987 - accuracy: 0.3000\n",
      "Epoch 93/100\n",
      "150/150 [==============================] - 0s 179us/step - loss: 1.0987 - accuracy: 0.3267\n",
      "Epoch 94/100\n",
      "150/150 [==============================] - 0s 173us/step - loss: 1.0987 - accuracy: 0.3333\n",
      "Epoch 95/100\n",
      "150/150 [==============================] - 0s 178us/step - loss: 1.0986 - accuracy: 0.3000\n",
      "Epoch 96/100\n",
      "150/150 [==============================] - 0s 173us/step - loss: 1.0986 - accuracy: 0.2867\n",
      "Epoch 97/100\n",
      "150/150 [==============================] - 0s 180us/step - loss: 1.0986 - accuracy: 0.3133\n",
      "Epoch 98/100\n",
      "150/150 [==============================] - 0s 173us/step - loss: 1.0987 - accuracy: 0.2933\n",
      "Epoch 99/100\n",
      "150/150 [==============================] - 0s 173us/step - loss: 1.0987 - accuracy: 0.2733\n",
      "Epoch 100/100\n",
      "150/150 [==============================] - 0s 173us/step - loss: 1.0987 - accuracy: 0.3133\n",
      "최적스코어 : 0.9800 사용한 파라미터 조합 : {'batch_size': 10, 'epochs': 100}\n",
      "0.66 (0.09) with{'batch_size': 10, 'epochs': 10}\n",
      "0.71 (0.29) with{'batch_size': 10, 'epochs': 50}\n",
      "0.98 (0.03) with{'batch_size': 10, 'epochs': 100}\n",
      "0.37 (0.22) with{'batch_size': 20, 'epochs': 10}\n",
      "0.73 (0.17) with{'batch_size': 20, 'epochs': 50}\n",
      "0.96 (0.02) with{'batch_size': 20, 'epochs': 100}\n",
      "0.49 (0.17) with{'batch_size': 30, 'epochs': 10}\n",
      "0.66 (0.12) with{'batch_size': 30, 'epochs': 50}\n",
      "0.78 (0.18) with{'batch_size': 30, 'epochs': 100}\n"
     ]
    }
   ],
   "source": [
    "#keras쪽 모델을 함수화 해서 받는다.\n",
    "model = KerasClassifier(build_fn = baseline_model, verbose=1) #build_fn 작성한 model function을 담는 매개변수\n",
    "batch_size=[10,20,30]\n",
    "epochs=[10,50,100] #3*3으로 9개의 조합으로 구성되어진다.\n",
    "param_grid=dict(batch_size=batch_size, epochs=epochs)\n",
    "grid=GridSearchCV(model, param_grid=param_grid, n_jobs=-1) #GridSearchCV에 함수모델을 내포한 model 을 넣을 수 있게된다.\n",
    "grid_result=grid.fit(X,Y)\n",
    "print('최적스코어 : {:.4f} 사용한 파라미터 조합 : {}'.format(grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, std, param in zip(means, stds, params):\n",
    "    #스코어별로 파라미터 조합을 볼 수 있다.\n",
    "    print('{:.2f} ({:.2f}) with{}'.format(mean, std, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 1.0864 - accuracy: 0.3400\n",
      "Epoch 2/100\n",
      "150/150 [==============================] - 0s 206us/step - loss: 1.0741 - accuracy: 0.3533\n",
      "Epoch 3/100\n",
      "150/150 [==============================] - 0s 199us/step - loss: 1.0643 - accuracy: 0.3733\n",
      "Epoch 4/100\n",
      "150/150 [==============================] - 0s 212us/step - loss: 1.0539 - accuracy: 0.5000\n",
      "Epoch 5/100\n",
      "150/150 [==============================] - 0s 245us/step - loss: 1.0436 - accuracy: 0.5867\n",
      "Epoch 6/100\n",
      "150/150 [==============================] - 0s 222us/step - loss: 1.0327 - accuracy: 0.6000\n",
      "Epoch 7/100\n",
      "150/150 [==============================] - 0s 220us/step - loss: 1.0219 - accuracy: 0.6067\n",
      "Epoch 8/100\n",
      "150/150 [==============================] - 0s 246us/step - loss: 1.0107 - accuracy: 0.6333\n",
      "Epoch 9/100\n",
      "150/150 [==============================] - 0s 221us/step - loss: 0.9994 - accuracy: 0.6467\n",
      "Epoch 10/100\n",
      "150/150 [==============================] - 0s 236us/step - loss: 0.9883 - accuracy: 0.6400\n",
      "Epoch 11/100\n",
      "150/150 [==============================] - 0s 193us/step - loss: 0.9768 - accuracy: 0.6533\n",
      "Epoch 12/100\n",
      "150/150 [==============================] - 0s 176us/step - loss: 0.9655 - accuracy: 0.6467\n",
      "Epoch 13/100\n",
      "150/150 [==============================] - 0s 166us/step - loss: 0.9540 - accuracy: 0.6533\n",
      "Epoch 14/100\n",
      "150/150 [==============================] - 0s 173us/step - loss: 0.9429 - accuracy: 0.6600\n",
      "Epoch 15/100\n",
      "150/150 [==============================] - 0s 193us/step - loss: 0.9318 - accuracy: 0.6600\n",
      "Epoch 16/100\n",
      "150/150 [==============================] - 0s 180us/step - loss: 0.9211 - accuracy: 0.6600\n",
      "Epoch 17/100\n",
      "150/150 [==============================] - 0s 206us/step - loss: 0.9102 - accuracy: 0.6267\n",
      "Epoch 18/100\n",
      "150/150 [==============================] - 0s 212us/step - loss: 0.8996 - accuracy: 0.5933\n",
      "Epoch 19/100\n",
      "150/150 [==============================] - 0s 206us/step - loss: 0.8894 - accuracy: 0.6667\n",
      "Epoch 20/100\n",
      "150/150 [==============================] - 0s 235us/step - loss: 0.8799 - accuracy: 0.6667\n",
      "Epoch 21/100\n",
      "150/150 [==============================] - 0s 222us/step - loss: 0.8694 - accuracy: 0.6667\n",
      "Epoch 22/100\n",
      "150/150 [==============================] - 0s 213us/step - loss: 0.8601 - accuracy: 0.6667\n",
      "Epoch 23/100\n",
      "150/150 [==============================] - 0s 262us/step - loss: 0.8506 - accuracy: 0.6667\n",
      "Epoch 24/100\n",
      "150/150 [==============================] - 0s 263us/step - loss: 0.8418 - accuracy: 0.6667\n",
      "Epoch 25/100\n",
      "150/150 [==============================] - 0s 246us/step - loss: 0.8332 - accuracy: 0.6667\n",
      "Epoch 26/100\n",
      "150/150 [==============================] - 0s 256us/step - loss: 0.8247 - accuracy: 0.6667\n",
      "Epoch 27/100\n",
      "150/150 [==============================] - 0s 233us/step - loss: 0.8168 - accuracy: 0.6667\n",
      "Epoch 28/100\n",
      "150/150 [==============================] - 0s 221us/step - loss: 0.8086 - accuracy: 0.6667\n",
      "Epoch 29/100\n",
      "150/150 [==============================] - 0s 220us/step - loss: 0.8013 - accuracy: 0.6667\n",
      "Epoch 30/100\n",
      "150/150 [==============================] - 0s 206us/step - loss: 0.7936 - accuracy: 0.6667\n",
      "Epoch 31/100\n",
      "150/150 [==============================] - 0s 199us/step - loss: 0.7865 - accuracy: 0.6667\n",
      "Epoch 32/100\n",
      "150/150 [==============================] - 0s 227us/step - loss: 0.7796 - accuracy: 0.6667\n",
      "Epoch 33/100\n",
      "150/150 [==============================] - 0s 392us/step - loss: 0.7729 - accuracy: 0.6667\n",
      "Epoch 34/100\n",
      "150/150 [==============================] - 0s 413us/step - loss: 0.7663 - accuracy: 0.6667\n",
      "Epoch 35/100\n",
      "150/150 [==============================] - 0s 399us/step - loss: 0.7598 - accuracy: 0.6667\n",
      "Epoch 36/100\n",
      "150/150 [==============================] - 0s 407us/step - loss: 0.7540 - accuracy: 0.6667\n",
      "Epoch 37/100\n",
      "150/150 [==============================] - 0s 374us/step - loss: 0.7482 - accuracy: 0.6667\n",
      "Epoch 38/100\n",
      "150/150 [==============================] - 0s 365us/step - loss: 0.7422 - accuracy: 0.6667\n",
      "Epoch 39/100\n",
      "150/150 [==============================] - 0s 424us/step - loss: 0.7365 - accuracy: 0.6667\n",
      "Epoch 40/100\n",
      "150/150 [==============================] - 0s 372us/step - loss: 0.7311 - accuracy: 0.6667\n",
      "Epoch 41/100\n",
      "150/150 [==============================] - 0s 379us/step - loss: 0.7255 - accuracy: 0.6667\n",
      "Epoch 42/100\n",
      "150/150 [==============================] - 0s 412us/step - loss: 0.7202 - accuracy: 0.6667\n",
      "Epoch 43/100\n",
      "150/150 [==============================] - 0s 366us/step - loss: 0.7148 - accuracy: 0.6667\n",
      "Epoch 44/100\n",
      "150/150 [==============================] - 0s 366us/step - loss: 0.7096 - accuracy: 0.6667\n",
      "Epoch 45/100\n",
      "150/150 [==============================] - 0s 414us/step - loss: 0.7046 - accuracy: 0.6667\n",
      "Epoch 46/100\n",
      "150/150 [==============================] - 0s 372us/step - loss: 0.7001 - accuracy: 0.6667\n",
      "Epoch 47/100\n",
      "150/150 [==============================] - 0s 435us/step - loss: 0.6951 - accuracy: 0.6667\n",
      "Epoch 48/100\n",
      "150/150 [==============================] - 0s 381us/step - loss: 0.6904 - accuracy: 0.6667\n",
      "Epoch 49/100\n",
      "150/150 [==============================] - 0s 357us/step - loss: 0.6859 - accuracy: 0.6667\n",
      "Epoch 50/100\n",
      "150/150 [==============================] - 0s 361us/step - loss: 0.6815 - accuracy: 0.6667\n",
      "Epoch 51/100\n",
      "150/150 [==============================] - 0s 429us/step - loss: 0.6771 - accuracy: 0.6667\n",
      "Epoch 52/100\n",
      "150/150 [==============================] - 0s 474us/step - loss: 0.6726 - accuracy: 0.6667\n",
      "Epoch 53/100\n",
      "150/150 [==============================] - 0s 412us/step - loss: 0.6685 - accuracy: 0.6667\n",
      "Epoch 54/100\n",
      "150/150 [==============================] - 0s 360us/step - loss: 0.6644 - accuracy: 0.6667\n",
      "Epoch 55/100\n",
      "150/150 [==============================] - 0s 611us/step - loss: 0.6605 - accuracy: 0.6667\n",
      "Epoch 56/100\n",
      "150/150 [==============================] - 0s 239us/step - loss: 0.6564 - accuracy: 0.6667\n",
      "Epoch 57/100\n",
      "150/150 [==============================] - 0s 180us/step - loss: 0.6525 - accuracy: 0.6667\n",
      "Epoch 58/100\n",
      "150/150 [==============================] - 0s 180us/step - loss: 0.6486 - accuracy: 0.6667\n",
      "Epoch 59/100\n",
      "150/150 [==============================] - 0s 148us/step - loss: 0.6453 - accuracy: 0.6667\n",
      "Epoch 60/100\n",
      "150/150 [==============================] - 0s 160us/step - loss: 0.6411 - accuracy: 0.6667\n",
      "Epoch 61/100\n",
      "150/150 [==============================] - 0s 207us/step - loss: 0.6379 - accuracy: 0.6667\n",
      "Epoch 62/100\n",
      "150/150 [==============================] - 0s 220us/step - loss: 0.6339 - accuracy: 0.6667\n",
      "Epoch 63/100\n",
      "150/150 [==============================] - 0s 205us/step - loss: 0.6306 - accuracy: 0.6667\n",
      "Epoch 64/100\n",
      "150/150 [==============================] - 0s 213us/step - loss: 0.6270 - accuracy: 0.6667\n",
      "Epoch 65/100\n",
      "150/150 [==============================] - 0s 212us/step - loss: 0.6239 - accuracy: 0.6667\n",
      "Epoch 66/100\n",
      "150/150 [==============================] - 0s 197us/step - loss: 0.6202 - accuracy: 0.6800\n",
      "Epoch 67/100\n",
      "150/150 [==============================] - 0s 187us/step - loss: 0.6171 - accuracy: 0.6800\n",
      "Epoch 68/100\n",
      "150/150 [==============================] - 0s 193us/step - loss: 0.6139 - accuracy: 0.6800\n",
      "Epoch 69/100\n",
      "150/150 [==============================] - 0s 197us/step - loss: 0.6106 - accuracy: 0.6800\n",
      "Epoch 70/100\n",
      "150/150 [==============================] - 0s 191us/step - loss: 0.6074 - accuracy: 0.6867\n",
      "Epoch 71/100\n",
      "150/150 [==============================] - 0s 200us/step - loss: 0.6043 - accuracy: 0.6867\n",
      "Epoch 72/100\n",
      "150/150 [==============================] - 0s 200us/step - loss: 0.6013 - accuracy: 0.6867\n",
      "Epoch 73/100\n",
      "150/150 [==============================] - 0s 183us/step - loss: 0.5984 - accuracy: 0.6867\n",
      "Epoch 74/100\n",
      "150/150 [==============================] - 0s 183us/step - loss: 0.5953 - accuracy: 0.6867\n",
      "Epoch 75/100\n",
      "150/150 [==============================] - 0s 199us/step - loss: 0.5924 - accuracy: 0.6867\n",
      "Epoch 76/100\n",
      "150/150 [==============================] - 0s 193us/step - loss: 0.5901 - accuracy: 0.6933\n",
      "Epoch 77/100\n",
      "150/150 [==============================] - 0s 193us/step - loss: 0.5868 - accuracy: 0.6867\n",
      "Epoch 78/100\n",
      "150/150 [==============================] - 0s 217us/step - loss: 0.5849 - accuracy: 0.6867\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - 0s 206us/step - loss: 0.5815 - accuracy: 0.6933\n",
      "Epoch 80/100\n",
      "150/150 [==============================] - 0s 166us/step - loss: 0.5786 - accuracy: 0.6933\n",
      "Epoch 81/100\n",
      "150/150 [==============================] - 0s 140us/step - loss: 0.5758 - accuracy: 0.6933\n",
      "Epoch 82/100\n",
      "150/150 [==============================] - 0s 140us/step - loss: 0.5736 - accuracy: 0.7000\n",
      "Epoch 83/100\n",
      "150/150 [==============================] - 0s 153us/step - loss: 0.5707 - accuracy: 0.7000\n",
      "Epoch 84/100\n",
      "150/150 [==============================] - 0s 199us/step - loss: 0.5681 - accuracy: 0.7000\n",
      "Epoch 85/100\n",
      "150/150 [==============================] - 0s 196us/step - loss: 0.5657 - accuracy: 0.7000\n",
      "Epoch 86/100\n",
      "150/150 [==============================] - 0s 286us/step - loss: 0.5632 - accuracy: 0.7067\n",
      "Epoch 87/100\n",
      "150/150 [==============================] - 0s 360us/step - loss: 0.5610 - accuracy: 0.7067\n",
      "Epoch 88/100\n",
      "150/150 [==============================] - 0s 366us/step - loss: 0.5582 - accuracy: 0.7067\n",
      "Epoch 89/100\n",
      "150/150 [==============================] - 0s 372us/step - loss: 0.5559 - accuracy: 0.7067\n",
      "Epoch 90/100\n",
      "150/150 [==============================] - 0s 373us/step - loss: 0.5537 - accuracy: 0.7067\n",
      "Epoch 91/100\n",
      "150/150 [==============================] - 0s 352us/step - loss: 0.5513 - accuracy: 0.7067\n",
      "Epoch 92/100\n",
      "150/150 [==============================] - 0s 349us/step - loss: 0.5491 - accuracy: 0.7067\n",
      "Epoch 93/100\n",
      "150/150 [==============================] - 0s 359us/step - loss: 0.5468 - accuracy: 0.7067\n",
      "Epoch 94/100\n",
      "150/150 [==============================] - 0s 309us/step - loss: 0.5444 - accuracy: 0.7067\n",
      "Epoch 95/100\n",
      "150/150 [==============================] - 0s 190us/step - loss: 0.5424 - accuracy: 0.7133\n",
      "Epoch 96/100\n",
      "150/150 [==============================] - 0s 213us/step - loss: 0.5401 - accuracy: 0.7133\n",
      "Epoch 97/100\n",
      "150/150 [==============================] - 0s 239us/step - loss: 0.5384 - accuracy: 0.7333\n",
      "Epoch 98/100\n",
      "150/150 [==============================] - 0s 254us/step - loss: 0.5360 - accuracy: 0.7333\n",
      "Epoch 99/100\n",
      "150/150 [==============================] - 0s 199us/step - loss: 0.5338 - accuracy: 0.7333\n",
      "Epoch 100/100\n",
      "150/150 [==============================] - 0s 193us/step - loss: 0.5316 - accuracy: 0.7333\n",
      "최적스코어 : 0.8467 , 사용한 파라메터 조합 : {'classifier__batch_size': 10, 'classifier__epochs': 100}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "estimators = []\n",
    "estimators.append(('MinMax', MinMaxScaler()))\n",
    "estimators.append(('classifier', KerasClassifier(build_fn=baseline_model, verbose=1))) #pipe라인 안에 KerasClassifier가 들어가있다.\n",
    "pipeline = Pipeline(estimators)\n",
    "param_grid = {'classifier__batch_size':batch_size, 'classifier__epochs':epochs} #pipeline과 gridSearchCV를 결합했을때\n",
    "#키의 매개변수에 접근하는 방식은 : 키이름__매개변수이름을 넣게된다.\n",
    "grid = GridSearchCV(pipeline, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(X,Y)\n",
    "print('최적스코어 : {:.4f} , 사용한 파라메터 조합 : {}'.format(grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#문제\n",
    "    # optimizer를 튜닝하시오 (어떤 놈이 좋은지) : Adam, RMSprop, SGD, Adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model(optimizer='adam'):\n",
    "    #model을 하나의 함수에 담았다.\n",
    "    model = Sequential()\n",
    "    model.add(Dense(8, input_dim=4, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 1.0983 - accuracy: 0.1000\n",
      "Epoch 2/100\n",
      "150/150 [==============================] - 0s 326us/step - loss: 1.0961 - accuracy: 0.2600\n",
      "Epoch 3/100\n",
      "150/150 [==============================] - 0s 306us/step - loss: 1.0893 - accuracy: 0.3800\n",
      "Epoch 4/100\n",
      "150/150 [==============================] - 0s 306us/step - loss: 1.0837 - accuracy: 0.3800\n",
      "Epoch 5/100\n",
      "150/150 [==============================] - 0s 300us/step - loss: 1.0787 - accuracy: 0.3333\n",
      "Epoch 6/100\n",
      "150/150 [==============================] - 0s 319us/step - loss: 1.0730 - accuracy: 0.3800\n",
      "Epoch 7/100\n",
      "150/150 [==============================] - 0s 326us/step - loss: 1.0672 - accuracy: 0.4933\n",
      "Epoch 8/100\n",
      "150/150 [==============================] - 0s 326us/step - loss: 1.0606 - accuracy: 0.5867\n",
      "Epoch 9/100\n",
      "150/150 [==============================] - 0s 300us/step - loss: 1.0537 - accuracy: 0.6533\n",
      "Epoch 10/100\n",
      "150/150 [==============================] - 0s 300us/step - loss: 1.0462 - accuracy: 0.6600\n",
      "Epoch 11/100\n",
      "150/150 [==============================] - 0s 332us/step - loss: 1.0382 - accuracy: 0.6600\n",
      "Epoch 12/100\n",
      "150/150 [==============================] - 0s 346us/step - loss: 1.0295 - accuracy: 0.6600\n",
      "Epoch 13/100\n",
      "150/150 [==============================] - 0s 332us/step - loss: 1.0204 - accuracy: 0.6600\n",
      "Epoch 14/100\n",
      "150/150 [==============================] - 0s 286us/step - loss: 1.0098 - accuracy: 0.6667\n",
      "Epoch 15/100\n",
      "150/150 [==============================] - 0s 293us/step - loss: 0.9994 - accuracy: 0.6667\n",
      "Epoch 16/100\n",
      "150/150 [==============================] - 0s 299us/step - loss: 0.9876 - accuracy: 0.6667\n",
      "Epoch 17/100\n",
      "150/150 [==============================] - 0s 319us/step - loss: 0.9755 - accuracy: 0.6600\n",
      "Epoch 18/100\n",
      "150/150 [==============================] - 0s 324us/step - loss: 0.9629 - accuracy: 0.6733\n",
      "Epoch 19/100\n",
      "150/150 [==============================] - 0s 325us/step - loss: 0.9494 - accuracy: 0.6600\n",
      "Epoch 20/100\n",
      "150/150 [==============================] - 0s 333us/step - loss: 0.9353 - accuracy: 0.6733\n",
      "Epoch 21/100\n",
      "150/150 [==============================] - 0s 306us/step - loss: 0.9208 - accuracy: 0.6733\n",
      "Epoch 22/100\n",
      "150/150 [==============================] - 0s 331us/step - loss: 0.9060 - accuracy: 0.7000\n",
      "Epoch 23/100\n",
      "150/150 [==============================] - 0s 321us/step - loss: 0.8906 - accuracy: 0.7133\n",
      "Epoch 24/100\n",
      "150/150 [==============================] - 0s 323us/step - loss: 0.8749 - accuracy: 0.7267\n",
      "Epoch 25/100\n",
      "150/150 [==============================] - 0s 292us/step - loss: 0.8590 - accuracy: 0.7400\n",
      "Epoch 26/100\n",
      "150/150 [==============================] - 0s 305us/step - loss: 0.8434 - accuracy: 0.7600\n",
      "Epoch 27/100\n",
      "150/150 [==============================] - 0s 306us/step - loss: 0.8270 - accuracy: 0.7867\n",
      "Epoch 28/100\n",
      "150/150 [==============================] - 0s 332us/step - loss: 0.8110 - accuracy: 0.7867\n",
      "Epoch 29/100\n",
      "150/150 [==============================] - 0s 322us/step - loss: 0.7950 - accuracy: 0.8200\n",
      "Epoch 30/100\n",
      "150/150 [==============================] - 0s 352us/step - loss: 0.7787 - accuracy: 0.8333\n",
      "Epoch 31/100\n",
      "150/150 [==============================] - 0s 320us/step - loss: 0.7634 - accuracy: 0.8533\n",
      "Epoch 32/100\n",
      "150/150 [==============================] - 0s 332us/step - loss: 0.7480 - accuracy: 0.8600\n",
      "Epoch 33/100\n",
      "150/150 [==============================] - 0s 336us/step - loss: 0.7338 - accuracy: 0.8400\n",
      "Epoch 34/100\n",
      "150/150 [==============================] - 0s 352us/step - loss: 0.7186 - accuracy: 0.8600\n",
      "Epoch 35/100\n",
      "150/150 [==============================] - 0s 320us/step - loss: 0.7045 - accuracy: 0.8533\n",
      "Epoch 36/100\n",
      "150/150 [==============================] - 0s 387us/step - loss: 0.6910 - accuracy: 0.8467\n",
      "Epoch 37/100\n",
      "150/150 [==============================] - 0s 688us/step - loss: 0.6774 - accuracy: 0.8600\n",
      "Epoch 38/100\n",
      "150/150 [==============================] - 0s 366us/step - loss: 0.6653 - accuracy: 0.8467\n",
      "Epoch 39/100\n",
      "150/150 [==============================] - 0s 632us/step - loss: 0.6525 - accuracy: 0.8467\n",
      "Epoch 40/100\n",
      "150/150 [==============================] - 0s 685us/step - loss: 0.6415 - accuracy: 0.8533\n",
      "Epoch 41/100\n",
      "150/150 [==============================] - 0s 632us/step - loss: 0.6299 - accuracy: 0.8533\n",
      "Epoch 42/100\n",
      "150/150 [==============================] - 0s 648us/step - loss: 0.6194 - accuracy: 0.8600\n",
      "Epoch 43/100\n",
      "150/150 [==============================] - 0s 684us/step - loss: 0.6088 - accuracy: 0.8600\n",
      "Epoch 44/100\n",
      "150/150 [==============================] - 0s 626us/step - loss: 0.5991 - accuracy: 0.8533\n",
      "Epoch 45/100\n",
      "150/150 [==============================] - 0s 386us/step - loss: 0.5898 - accuracy: 0.8533\n",
      "Epoch 46/100\n",
      "150/150 [==============================] - 0s 346us/step - loss: 0.5807 - accuracy: 0.8533\n",
      "Epoch 47/100\n",
      "150/150 [==============================] - 0s 359us/step - loss: 0.5721 - accuracy: 0.8667\n",
      "Epoch 48/100\n",
      "150/150 [==============================] - 0s 372us/step - loss: 0.5635 - accuracy: 0.8867\n",
      "Epoch 49/100\n",
      "150/150 [==============================] - 0s 375us/step - loss: 0.5552 - accuracy: 0.8800\n",
      "Epoch 50/100\n",
      "150/150 [==============================] - 0s 340us/step - loss: 0.5481 - accuracy: 0.8533\n",
      "Epoch 51/100\n",
      "150/150 [==============================] - 0s 352us/step - loss: 0.5410 - accuracy: 0.8933\n",
      "Epoch 52/100\n",
      "150/150 [==============================] - 0s 339us/step - loss: 0.5344 - accuracy: 0.8933\n",
      "Epoch 53/100\n",
      "150/150 [==============================] - 0s 399us/step - loss: 0.5260 - accuracy: 0.8800\n",
      "Epoch 54/100\n",
      "150/150 [==============================] - 0s 406us/step - loss: 0.5193 - accuracy: 0.8667\n",
      "Epoch 55/100\n",
      "150/150 [==============================] - 0s 382us/step - loss: 0.5131 - accuracy: 0.9000\n",
      "Epoch 56/100\n",
      "150/150 [==============================] - 0s 333us/step - loss: 0.5064 - accuracy: 0.9067\n",
      "Epoch 57/100\n",
      "150/150 [==============================] - 0s 472us/step - loss: 0.5007 - accuracy: 0.9133\n",
      "Epoch 58/100\n",
      "150/150 [==============================] - 0s 545us/step - loss: 0.4943 - accuracy: 0.9200\n",
      "Epoch 59/100\n",
      "150/150 [==============================] - 0s 574us/step - loss: 0.4892 - accuracy: 0.9200\n",
      "Epoch 60/100\n",
      "150/150 [==============================] - 0s 572us/step - loss: 0.4828 - accuracy: 0.9267\n",
      "Epoch 61/100\n",
      "150/150 [==============================] - 0s 432us/step - loss: 0.4776 - accuracy: 0.9067\n",
      "Epoch 62/100\n",
      "150/150 [==============================] - 0s 329us/step - loss: 0.4719 - accuracy: 0.9267\n",
      "Epoch 63/100\n",
      "150/150 [==============================] - 0s 369us/step - loss: 0.4670 - accuracy: 0.9267\n",
      "Epoch 64/100\n",
      "150/150 [==============================] - 0s 394us/step - loss: 0.4620 - accuracy: 0.9333\n",
      "Epoch 65/100\n",
      "150/150 [==============================] - 0s 477us/step - loss: 0.4568 - accuracy: 0.9333\n",
      "Epoch 66/100\n",
      "150/150 [==============================] - 0s 519us/step - loss: 0.4519 - accuracy: 0.9333\n",
      "Epoch 67/100\n",
      "150/150 [==============================] - 0s 552us/step - loss: 0.4471 - accuracy: 0.9333\n",
      "Epoch 68/100\n",
      "150/150 [==============================] - 0s 372us/step - loss: 0.4423 - accuracy: 0.9333\n",
      "Epoch 69/100\n",
      "150/150 [==============================] - 0s 507us/step - loss: 0.4377 - accuracy: 0.9333\n",
      "Epoch 70/100\n",
      "150/150 [==============================] - 0s 506us/step - loss: 0.4333 - accuracy: 0.9333\n",
      "Epoch 71/100\n",
      "150/150 [==============================] - 0s 439us/step - loss: 0.4297 - accuracy: 0.9467\n",
      "Epoch 72/100\n",
      "150/150 [==============================] - 0s 554us/step - loss: 0.4250 - accuracy: 0.9533\n",
      "Epoch 73/100\n",
      "150/150 [==============================] - 0s 353us/step - loss: 0.4206 - accuracy: 0.9400\n",
      "Epoch 74/100\n",
      "150/150 [==============================] - 0s 515us/step - loss: 0.4167 - accuracy: 0.9467\n",
      "Epoch 75/100\n",
      "150/150 [==============================] - 0s 589us/step - loss: 0.4129 - accuracy: 0.9533\n",
      "Epoch 76/100\n",
      "150/150 [==============================] - 0s 597us/step - loss: 0.4082 - accuracy: 0.9400\n",
      "Epoch 77/100\n",
      "150/150 [==============================] - 0s 598us/step - loss: 0.4041 - accuracy: 0.9467\n",
      "Epoch 78/100\n",
      "150/150 [==============================] - 0s 619us/step - loss: 0.4000 - accuracy: 0.9533\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - 0s 549us/step - loss: 0.3970 - accuracy: 0.9600\n",
      "Epoch 80/100\n",
      "150/150 [==============================] - 0s 366us/step - loss: 0.3929 - accuracy: 0.9600\n",
      "Epoch 81/100\n",
      "150/150 [==============================] - 0s 545us/step - loss: 0.3896 - accuracy: 0.9400\n",
      "Epoch 82/100\n",
      "150/150 [==============================] - 0s 351us/step - loss: 0.3852 - accuracy: 0.9467\n",
      "Epoch 83/100\n",
      "150/150 [==============================] - 0s 348us/step - loss: 0.3820 - accuracy: 0.9600\n",
      "Epoch 84/100\n",
      "150/150 [==============================] - 0s 551us/step - loss: 0.3780 - accuracy: 0.9600\n",
      "Epoch 85/100\n",
      "150/150 [==============================] - 0s 552us/step - loss: 0.3743 - accuracy: 0.9600\n",
      "Epoch 86/100\n",
      "150/150 [==============================] - 0s 605us/step - loss: 0.3716 - accuracy: 0.9533\n",
      "Epoch 87/100\n",
      "150/150 [==============================] - 0s 612us/step - loss: 0.3687 - accuracy: 0.9600\n",
      "Epoch 88/100\n",
      "150/150 [==============================] - 0s 582us/step - loss: 0.3647 - accuracy: 0.9533\n",
      "Epoch 89/100\n",
      "150/150 [==============================] - 0s 646us/step - loss: 0.3607 - accuracy: 0.9533\n",
      "Epoch 90/100\n",
      "150/150 [==============================] - 0s 625us/step - loss: 0.3577 - accuracy: 0.9600\n",
      "Epoch 91/100\n",
      "150/150 [==============================] - 0s 639us/step - loss: 0.3545 - accuracy: 0.9600\n",
      "Epoch 92/100\n",
      "150/150 [==============================] - 0s 618us/step - loss: 0.3509 - accuracy: 0.9600\n",
      "Epoch 93/100\n",
      "150/150 [==============================] - 0s 679us/step - loss: 0.3485 - accuracy: 0.9600\n",
      "Epoch 94/100\n",
      "150/150 [==============================] - 0s 706us/step - loss: 0.3452 - accuracy: 0.9600\n",
      "Epoch 95/100\n",
      "150/150 [==============================] - 0s 692us/step - loss: 0.3416 - accuracy: 0.9600\n",
      "Epoch 96/100\n",
      "150/150 [==============================] - 0s 660us/step - loss: 0.3383 - accuracy: 0.9600\n",
      "Epoch 97/100\n",
      "150/150 [==============================] - 0s 658us/step - loss: 0.3351 - accuracy: 0.9600\n",
      "Epoch 98/100\n",
      "150/150 [==============================] - 0s 554us/step - loss: 0.3326 - accuracy: 0.9600\n",
      "Epoch 99/100\n",
      "150/150 [==============================] - 0s 539us/step - loss: 0.3304 - accuracy: 0.9600\n",
      "Epoch 100/100\n",
      "150/150 [==============================] - 0s 559us/step - loss: 0.3263 - accuracy: 0.9600\n",
      "최적스코어 : 0.8867 , 사용한 파라메터 조합 : {'classifier__batch_size': 10, 'classifier__epochs': 100, 'classifier__optimizer': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "estimators = []\n",
    "epochs = [10,50,100]\n",
    "batches = [10,20,30]\n",
    "optimizers = ['rmsprop', 'adam', 'SGD']\n",
    "\n",
    "estimators.append(('MinMax', MinMaxScaler()))\n",
    "estimators.append(('classifier', KerasClassifier(build_fn=baseline_model, verbose=1)))\n",
    "pipeline = Pipeline(estimators)\n",
    "param_grid = {'classifier__batch_size':batches, 'classifier__epochs':epochs, 'classifier__optimizer':optimizers} \n",
    "\n",
    "grid = GridSearchCV(pipeline, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(X,Y)\n",
    "print('최적스코어 : {:.4f} , 사용한 파라메터 조합 : {}'.format(grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 1.1127 - accuracy: 0.2800\n",
      "Epoch 2/100\n",
      "150/150 [==============================] - 0s 279us/step - loss: 1.0926 - accuracy: 0.2533\n",
      "Epoch 3/100\n",
      "150/150 [==============================] - 0s 387us/step - loss: 1.0814 - accuracy: 0.2800\n",
      "Epoch 4/100\n",
      "150/150 [==============================] - 0s 326us/step - loss: 1.0709 - accuracy: 0.2733\n",
      "Epoch 5/100\n",
      "150/150 [==============================] - 0s 330us/step - loss: 1.0595 - accuracy: 0.3267\n",
      "Epoch 6/100\n",
      "150/150 [==============================] - 0s 308us/step - loss: 1.0477 - accuracy: 0.3933\n",
      "Epoch 7/100\n",
      "150/150 [==============================] - 0s 339us/step - loss: 1.0370 - accuracy: 0.6200\n",
      "Epoch 8/100\n",
      "150/150 [==============================] - 0s 321us/step - loss: 1.0258 - accuracy: 0.6133\n",
      "Epoch 9/100\n",
      "150/150 [==============================] - 0s 319us/step - loss: 1.0152 - accuracy: 0.6533\n",
      "Epoch 10/100\n",
      "150/150 [==============================] - 0s 287us/step - loss: 1.0022 - accuracy: 0.6467\n",
      "Epoch 11/100\n",
      "150/150 [==============================] - 0s 352us/step - loss: 0.9928 - accuracy: 0.6133\n",
      "Epoch 12/100\n",
      "150/150 [==============================] - 0s 308us/step - loss: 0.9799 - accuracy: 0.6133\n",
      "Epoch 13/100\n",
      "150/150 [==============================] - 0s 327us/step - loss: 0.9675 - accuracy: 0.6533\n",
      "Epoch 14/100\n",
      "150/150 [==============================] - 0s 327us/step - loss: 0.9554 - accuracy: 0.6133\n",
      "Epoch 15/100\n",
      "150/150 [==============================] - 0s 326us/step - loss: 0.9436 - accuracy: 0.6400\n",
      "Epoch 16/100\n",
      "150/150 [==============================] - 0s 361us/step - loss: 0.9308 - accuracy: 0.5867\n",
      "Epoch 17/100\n",
      "150/150 [==============================] - 0s 333us/step - loss: 0.9174 - accuracy: 0.6200\n",
      "Epoch 18/100\n",
      "150/150 [==============================] - 0s 315us/step - loss: 0.9043 - accuracy: 0.6200\n",
      "Epoch 19/100\n",
      "150/150 [==============================] - 0s 359us/step - loss: 0.8910 - accuracy: 0.6067\n",
      "Epoch 20/100\n",
      "150/150 [==============================] - 0s 565us/step - loss: 0.8770 - accuracy: 0.6000\n",
      "Epoch 21/100\n",
      "150/150 [==============================] - 0s 559us/step - loss: 0.8637 - accuracy: 0.5867\n",
      "Epoch 22/100\n",
      "150/150 [==============================] - 0s 582us/step - loss: 0.8511 - accuracy: 0.5933\n",
      "Epoch 23/100\n",
      "150/150 [==============================] - 0s 583us/step - loss: 0.8371 - accuracy: 0.6067\n",
      "Epoch 24/100\n",
      "150/150 [==============================] - 0s 583us/step - loss: 0.8237 - accuracy: 0.6200\n",
      "Epoch 25/100\n",
      "150/150 [==============================] - 0s 594us/step - loss: 0.8100 - accuracy: 0.5867\n",
      "Epoch 26/100\n",
      "150/150 [==============================] - 0s 579us/step - loss: 0.7949 - accuracy: 0.4867\n",
      "Epoch 27/100\n",
      "150/150 [==============================] - 0s 581us/step - loss: 0.7824 - accuracy: 0.6000\n",
      "Epoch 28/100\n",
      "150/150 [==============================] - 0s 552us/step - loss: 0.7696 - accuracy: 0.5067\n",
      "Epoch 29/100\n",
      "150/150 [==============================] - 0s 588us/step - loss: 0.7551 - accuracy: 0.5000\n",
      "Epoch 30/100\n",
      "150/150 [==============================] - 0s 626us/step - loss: 0.7441 - accuracy: 0.5333\n",
      "Epoch 31/100\n",
      "150/150 [==============================] - 0s 576us/step - loss: 0.7308 - accuracy: 0.4533\n",
      "Epoch 32/100\n",
      "150/150 [==============================] - 0s 606us/step - loss: 0.7175 - accuracy: 0.5600\n",
      "Epoch 33/100\n",
      "150/150 [==============================] - 0s 593us/step - loss: 0.7059 - accuracy: 0.4733\n",
      "Epoch 34/100\n",
      "150/150 [==============================] - 0s 623us/step - loss: 0.6936 - accuracy: 0.4467\n",
      "Epoch 35/100\n",
      "150/150 [==============================] - 0s 593us/step - loss: 0.6829 - accuracy: 0.3867\n",
      "Epoch 36/100\n",
      "150/150 [==============================] - 0s 565us/step - loss: 0.6711 - accuracy: 0.4067\n",
      "Epoch 37/100\n",
      "150/150 [==============================] - 0s 693us/step - loss: 0.6602 - accuracy: 0.4867\n",
      "Epoch 38/100\n",
      "150/150 [==============================] - 0s 671us/step - loss: 0.6496 - accuracy: 0.4533\n",
      "Epoch 39/100\n",
      "150/150 [==============================] - 0s 639us/step - loss: 0.6394 - accuracy: 0.5267\n",
      "Epoch 40/100\n",
      "150/150 [==============================] - 0s 592us/step - loss: 0.6298 - accuracy: 0.4067\n",
      "Epoch 41/100\n",
      "150/150 [==============================] - 0s 552us/step - loss: 0.6205 - accuracy: 0.4200\n",
      "Epoch 42/100\n",
      "150/150 [==============================] - 0s 805us/step - loss: 0.6113 - accuracy: 0.5333\n",
      "Epoch 43/100\n",
      "150/150 [==============================] - 0s 667us/step - loss: 0.6022 - accuracy: 0.4933\n",
      "Epoch 44/100\n",
      "150/150 [==============================] - 0s 671us/step - loss: 0.5937 - accuracy: 0.5667\n",
      "Epoch 45/100\n",
      "150/150 [==============================] - 0s 646us/step - loss: 0.5846 - accuracy: 0.5867\n",
      "Epoch 46/100\n",
      "150/150 [==============================] - 0s 638us/step - loss: 0.5777 - accuracy: 0.4667\n",
      "Epoch 47/100\n",
      "150/150 [==============================] - 0s 644us/step - loss: 0.5703 - accuracy: 0.6667\n",
      "Epoch 48/100\n",
      "150/150 [==============================] - 0s 656us/step - loss: 0.5629 - accuracy: 0.6267\n",
      "Epoch 49/100\n",
      "150/150 [==============================] - 0s 671us/step - loss: 0.5576 - accuracy: 0.6333\n",
      "Epoch 50/100\n",
      "150/150 [==============================] - 0s 658us/step - loss: 0.5509 - accuracy: 0.6533\n",
      "Epoch 51/100\n",
      "150/150 [==============================] - 0s 715us/step - loss: 0.5456 - accuracy: 0.6467\n",
      "Epoch 52/100\n",
      "150/150 [==============================] - 0s 638us/step - loss: 0.5395 - accuracy: 0.6533\n",
      "Epoch 53/100\n",
      "150/150 [==============================] - 0s 689us/step - loss: 0.5337 - accuracy: 0.6667\n",
      "Epoch 54/100\n",
      "150/150 [==============================] - 0s 636us/step - loss: 0.5297 - accuracy: 0.6667\n",
      "Epoch 55/100\n",
      "150/150 [==============================] - 0s 646us/step - loss: 0.5233 - accuracy: 0.6667\n",
      "Epoch 56/100\n",
      "150/150 [==============================] - 0s 639us/step - loss: 0.5203 - accuracy: 0.6667\n",
      "Epoch 57/100\n",
      "150/150 [==============================] - 0s 620us/step - loss: 0.5144 - accuracy: 0.6667\n",
      "Epoch 58/100\n",
      "150/150 [==============================] - 0s 672us/step - loss: 0.5104 - accuracy: 0.6667\n",
      "Epoch 59/100\n",
      "150/150 [==============================] - 0s 665us/step - loss: 0.5053 - accuracy: 0.6667\n",
      "Epoch 60/100\n",
      "150/150 [==============================] - 0s 638us/step - loss: 0.5019 - accuracy: 0.6667\n",
      "Epoch 61/100\n",
      "150/150 [==============================] - 0s 625us/step - loss: 0.4967 - accuracy: 0.6667\n",
      "Epoch 62/100\n",
      "150/150 [==============================] - 0s 646us/step - loss: 0.4931 - accuracy: 0.6667\n",
      "Epoch 63/100\n",
      "150/150 [==============================] - 0s 641us/step - loss: 0.4889 - accuracy: 0.6667\n",
      "Epoch 64/100\n",
      "150/150 [==============================] - 0s 645us/step - loss: 0.4874 - accuracy: 0.6667\n",
      "Epoch 65/100\n",
      "150/150 [==============================] - 0s 640us/step - loss: 0.4829 - accuracy: 0.6667\n",
      "Epoch 66/100\n",
      "150/150 [==============================] - 0s 645us/step - loss: 0.4799 - accuracy: 0.6733\n",
      "Epoch 67/100\n",
      "150/150 [==============================] - 0s 613us/step - loss: 0.4773 - accuracy: 0.6667\n",
      "Epoch 68/100\n",
      "150/150 [==============================] - 0s 725us/step - loss: 0.4740 - accuracy: 0.6667\n",
      "Epoch 69/100\n",
      "150/150 [==============================] - 0s 733us/step - loss: 0.4715 - accuracy: 0.6733\n",
      "Epoch 70/100\n",
      "150/150 [==============================] - 0s 731us/step - loss: 0.4687 - accuracy: 0.6667\n",
      "Epoch 71/100\n",
      "150/150 [==============================] - 0s 671us/step - loss: 0.4666 - accuracy: 0.6667\n",
      "Epoch 72/100\n",
      "150/150 [==============================] - 0s 660us/step - loss: 0.4630 - accuracy: 0.6667\n",
      "Epoch 73/100\n",
      "150/150 [==============================] - 0s 659us/step - loss: 0.4611 - accuracy: 0.6800\n",
      "Epoch 74/100\n",
      "150/150 [==============================] - 0s 674us/step - loss: 0.4585 - accuracy: 0.6667\n",
      "Epoch 75/100\n",
      "150/150 [==============================] - 0s 649us/step - loss: 0.4562 - accuracy: 0.6800\n",
      "Epoch 76/100\n",
      "150/150 [==============================] - 0s 667us/step - loss: 0.4537 - accuracy: 0.6667\n",
      "Epoch 77/100\n",
      "150/150 [==============================] - 0s 706us/step - loss: 0.4517 - accuracy: 0.6667\n",
      "Epoch 78/100\n",
      "150/150 [==============================] - 0s 629us/step - loss: 0.4494 - accuracy: 0.6867\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - 0s 675us/step - loss: 0.4465 - accuracy: 0.6867\n",
      "Epoch 80/100\n",
      "150/150 [==============================] - 0s 642us/step - loss: 0.4453 - accuracy: 0.6733\n",
      "Epoch 81/100\n",
      "150/150 [==============================] - 0s 612us/step - loss: 0.4439 - accuracy: 0.7067\n",
      "Epoch 82/100\n",
      "150/150 [==============================] - 0s 610us/step - loss: 0.4409 - accuracy: 0.7000\n",
      "Epoch 83/100\n",
      "150/150 [==============================] - 0s 635us/step - loss: 0.4385 - accuracy: 0.6933\n",
      "Epoch 84/100\n",
      "150/150 [==============================] - 0s 633us/step - loss: 0.4368 - accuracy: 0.6933\n",
      "Epoch 85/100\n",
      "150/150 [==============================] - 0s 639us/step - loss: 0.4346 - accuracy: 0.7000\n",
      "Epoch 86/100\n",
      "150/150 [==============================] - 0s 609us/step - loss: 0.4329 - accuracy: 0.7067\n",
      "Epoch 87/100\n",
      "150/150 [==============================] - 0s 625us/step - loss: 0.4325 - accuracy: 0.7267\n",
      "Epoch 88/100\n",
      "150/150 [==============================] - 0s 618us/step - loss: 0.4290 - accuracy: 0.7200\n",
      "Epoch 89/100\n",
      "150/150 [==============================] - 0s 639us/step - loss: 0.4271 - accuracy: 0.7400\n",
      "Epoch 90/100\n",
      "150/150 [==============================] - 0s 612us/step - loss: 0.4248 - accuracy: 0.7467\n",
      "Epoch 91/100\n",
      "150/150 [==============================] - 0s 726us/step - loss: 0.4225 - accuracy: 0.7400\n",
      "Epoch 92/100\n",
      "150/150 [==============================] - 0s 617us/step - loss: 0.4211 - accuracy: 0.7200\n",
      "Epoch 93/100\n",
      "150/150 [==============================] - 0s 612us/step - loss: 0.4193 - accuracy: 0.7733\n",
      "Epoch 94/100\n",
      "150/150 [==============================] - 0s 639us/step - loss: 0.4167 - accuracy: 0.8200\n",
      "Epoch 95/100\n",
      "150/150 [==============================] - 0s 627us/step - loss: 0.4145 - accuracy: 0.8067\n",
      "Epoch 96/100\n",
      "150/150 [==============================] - 0s 612us/step - loss: 0.4123 - accuracy: 0.7467\n",
      "Epoch 97/100\n",
      "150/150 [==============================] - 0s 648us/step - loss: 0.4111 - accuracy: 0.8267\n",
      "Epoch 98/100\n",
      "150/150 [==============================] - 0s 672us/step - loss: 0.4088 - accuracy: 0.8333\n",
      "Epoch 99/100\n",
      "150/150 [==============================] - 0s 695us/step - loss: 0.4065 - accuracy: 0.8400\n",
      "Epoch 100/100\n",
      "150/150 [==============================] - 0s 758us/step - loss: 0.4052 - accuracy: 0.8133\n",
      "최적스코어 : 0.9733 , 사용한 파라메터 조합 : {'batch_size': 10, 'epochs': 100, 'optimizer': 'rmsprop'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "estimators = KerasClassifier(build_fn=baseline_model, verbose=1)\n",
    "\n",
    "epochs = [10,50,100]\n",
    "batches = [10,20,30]\n",
    "optimizers = ['rmsprop', 'adam', 'SGD']\n",
    "params = dict(epochs=epochs, batch_size=batches, optimizer=optimizers)\n",
    "\n",
    "grid = GridSearchCV(estimator=estimators, param_grid=params, n_jobs=-1)\n",
    "grid_result = grid.fit(X,Y)\n",
    "print('최적스코어 : {:.4f} , 사용한 파라메터 조합 : {}'.format(grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_features = 100\n",
    "features, target = make_classification(n_samples = 10000,\n",
    "                                       n_features = number_of_features,\n",
    "                                       n_informative = 3,\n",
    "                                       n_redundant = 0,\n",
    "                                       n_classes = 2,\n",
    "                                       weights = [.5, .5],\n",
    "                                       random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_network(optimizer='rmsprop'):\n",
    "    network = models.Sequential()\n",
    "    network.add(layers.Dense(units=16, activation='relu', input_shape=(number_of_features,)))\n",
    "    network.add(layers.Dense(units=16, activation='relu'))\n",
    "    network.add(layers.Dense(units=1, activation='sigmoid'))\n",
    "    network.compile(loss='binary_crossentropy', # Cross-entropy\n",
    "                    optimizer=optimizer, # Optimizer\n",
    "                    metrics=['accuracy']) # Accuracy performance metric\n",
    "    return network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap Keras model so it can be used by scikit-learn\n",
    "neural_network = KerasClassifier(build_fn=create_network, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = [5, 10]\n",
    "batches = [5, 10, 100]\n",
    "optimizers = ['rmsprop', 'adam']\n",
    "\n",
    "hyperparameters = dict(optimizer=optimizers, epochs=epochs, batch_size=batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "grid = GridSearchCV(estimator=neural_network, cv=3, param_grid=hyperparameters)\n",
    "\n",
    "grid_result = grid.fit(features, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 10, 'epochs': 5, 'optimizer': 'adam'}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_result.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
