{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cnn convolution neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10 #분류종류 10개\n",
    "learning_rate = 0.001 #hyper parameter\n",
    "training_steps = 200 #epoch(한번에 로드되는 데이터의 갯수)\n",
    "batch_size = 128\n",
    "display_step = 10\n",
    "\n",
    "conv1_filters = 32 #학습대상 - 하나의 이미지에서 32개의 특징을 추출한다.\n",
    "conv2_filters = 64 \n",
    "fc1_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = np.array(x_train, np.float32), np.array(x_test, np.float32)\n",
    "x_train, x_test = x_train/255., x_test/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.data.Dataset.from_tensor_slices((x_train,y_train))\n",
    "train_data = train_data.repeat().shuffle(5000).batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train)) #from_tensor_slices - 일정한 사이즈로 데이터를 쪼갠다.\n",
    "\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME') \n",
    "    #strides - 건너뛰는 갯수, padding - 입력차수, 출력차수 동일\n",
    "    x = tf.nn.bias_add(x,b) #y = ax+b (bias_add - 바이어스의 상,하한치를 고려해준다.)\n",
    "    return tf.nn.relu(x)\n",
    "def maxpool2d(x, k=2):\n",
    "    #ksize - 커널 사이즈 (1/4로 줄어든다.)\n",
    "    return tf.nn.max_pool(x, ksize=[1,k,k,1], strides=[1,k,k,1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_normal = tf.initializers.RandomNormal() #함수 인스턴스\n",
    "weights = {\n",
    "    #5*5 = filter size, 1->입력채널수, conv1_filters->출력차수\n",
    "    'wc1' : tf.Variable(random_normal([5,5,1,conv1_filters])),\n",
    "    'wc2' : tf.Variable(random_normal([5,5,conv1_filters, conv2_filters])),\n",
    "    #wd = dense망의 가중치\n",
    "    'wd1' : tf.Variable(random_normal([7*7*64, fc1_units])),\n",
    "    'out' : tf.Variable(random_normal([fc1_units, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    #나가는 차수에 맞춰서 생성된다.\n",
    "    'bc1' : tf.Variable(tf.zeros([conv1_filters])),\n",
    "    'bc2' : tf.Variable(tf.zeros([conv2_filters])),\n",
    "    'bd1' : tf.Variable(tf.zeros([fc1_units])),\n",
    "    'out' : tf.Variable(tf.zeros([num_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_net(x):\n",
    "    x = tf.reshape(x, [-1, 28, 28, 1]) #128*28*28*1\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1']) #128*28*28*32\n",
    "    conv1 = maxpool2d(conv1, k=2) #128*14*14*32\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2']) #128*14*14*64\n",
    "    conv2 = maxpool2d(conv2, k=2) #128*7*7*64\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]]) #128*1024\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out']) #128*10\n",
    "    return tf.nn.softmax(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y_pred, y_true):\n",
    "    #예측값과 실제값\n",
    "    y_true = tf.one_hot(y_true, depth=num_classes)\n",
    "    y_pred = tf.clip_by_value(y_pred, 1e-9, 1.) #clip_by_value - 값에 제한을 주는것\n",
    "    return tf.reduce_mean(-tf.reduce_sum(y_true*tf.math.log(y_pred)))\n",
    "def accuracy(y_pred, y_true):\n",
    "    correct_prediction=tf.equal(tf.argmax(y_pred, 1), tf.cast(y_true, tf.int64)) #equal 비교연산자 결과값 비교(boolean값으로 리턴)\n",
    "    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32), axis=-1)\n",
    "optimizer = tf.optimizers.Adam(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_optimization(x,y):\n",
    "    with tf.GradientTape() as g:\n",
    "        #미분에 대상이 되는값을 저장한다.\n",
    "        pred = conv_net(x) #통과하면 분류예측값이 된다.(0~1사이의 확률값으로 나옴)\n",
    "        loss = cross_entropy(pred, y) #loss값 비용값 산정\n",
    "    train_variables = list(weights.values())+list(biases.values())\n",
    "    grdients = g.gradient(loss, train_variables) #학습의 대상이 된다. (편미분)\n",
    "    #여러개의 변수로 구성된 식에서 하나면 변수로 보고 나머지는 상수로 취급\n",
    "    optimizer.apply_gradients(zip(grdients, train_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step, (batch_x, batch_y) in enumerate(train_data.take(training_steps), 1):\n",
    "    run_optimization(batch_x, batch_y) #함수가 계층적으로 호출(내부적으로는 그래프를 만들어서 실행된다.)\n",
    "    if step % display_step == 0:\n",
    "        #배수를 찾아서 출력해라\n",
    "        pred = conv_net(batch_x)\n",
    "        loss = cross_entropy(pred, batch_y)\n",
    "        acc = accuracy(pred, batch_y)\n",
    "        print('step :  %i, loss : %f, accuracy : %f'%(step, loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#문제 :테스트 accuracy 를 출력하시오\n",
    "    #test 데이터 5장을 출력하고 예측을 출력하시오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = conv_net(x_test)\n",
    "acc = accuracy(pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "n_image = 5\n",
    "test_image = x_test[:n_image]\n",
    "pred = conv_net(test_image)\n",
    "\n",
    "for i in range(n_image):\n",
    "    plt.imshow(np.reshape(test_image[i], [28,28]), cmap='gray')\n",
    "    plt.show()\n",
    "    print('모델예측결과 : %i'%(tf.argmax(pred[i]).numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keras의 모델을 상속받아서 구현해보아라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model, layers\n",
    "\n",
    "class ConvNet(Model):\n",
    "    #원래 생성자 호출은 불가능한데 예외가 있다 상속받은 경우에는 부모의 생성자를 직접 호출할 수 있다.\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = layers.Conv2D(32, kernel_size=5, activation=tf.nn.relu)\n",
    "        self.maxpool1 = layers.MaxPool2D(2, strides=2)\n",
    "        self.conv2 = layers.Conv2D(64, kernel_size=3, activation=tf.nn.relu)\n",
    "        self.maxpool2 = layers.MaxPool2D(2, strides=2)\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.fc1 = layers.Dense(1024)\n",
    "        self.dropout = layers.Dropout(rate = 0.5)\n",
    "        self.out = layers.Dense(num_classes)\n",
    "\n",
    "    def call(self, x, is_training = False):\n",
    "        x = tf.reshape(x, [-1, 28, 28, 1])\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(x, training=is_training)\n",
    "        x = self.out(x)\n",
    "        if not is_training:\n",
    "            x=tf.nn.softmax(x)\n",
    "        return x\n",
    "conv_net = ConvNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(x,y):\n",
    "    y = tf.cast(y, tf.int64)\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=x)\n",
    "    return tf.reduce_mean(loss)\n",
    "def accuracy(y_pred, y_true):\n",
    "    correct_prediction = tf.equal(tf.argmax(y_pred,1), tf.cast(y_true, tf.int64))\n",
    "    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32), axis=-1)\n",
    "optimizer = tf.optimizers.Adam(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_optimization(x, y):\n",
    "    with tf.GradientTape() as g:\n",
    "        pred = conv_net(x, is_training =True)\n",
    "        loss = cross_entropy_loss(pred, y)\n",
    "    trainable_variables = conv_net.trainable_variables\n",
    "    gradients = g.gradient(loss, trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step, (batch_x, batch_y) in enumerate(train_data.take(training_steps),1):\n",
    "    run_optimization(batch_x, batch_y)\n",
    "    if step % display_step == 0 :\n",
    "        pred = conv_net(batch_x)\n",
    "        loss = cross_entropy_loss(pred, batch_y)\n",
    "        acc = accuracy(pred, batch_y)\n",
    "        print('step: %i, loss: %f, accuracy: %f'%(step, loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_path = '/tmp/logs/'\n",
    "num_classes = 10\n",
    "num_features = 784\n",
    "learning_rate = 0.001\n",
    "training_steps = 3000\n",
    "batch_size = 256\n",
    "display_step = 100\n",
    "n_hidden_1 = 128\n",
    "n_hidden_2 = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = np.array(x_train, np.float32), np.array(x_test, np.float32)\n",
    "x_train, x_test = x_train.reshape([-1, num_features]), x_test.reshape([-1, num_features])\n",
    "x_train, x_test = x_train/255., x_test/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_data = train_data.repeat().shuffle(5000).batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_normal = tf.initializers.RandomNormal()\n",
    "weights={\n",
    "    'h1_weights' : tf.Variable(random_normal([num_features, n_hidden_1]), name='h1_weights'),\n",
    "    'h2_weights' : tf.Variable(random_normal([n_hidden_1, n_hidden_2]), name='h2_weights'),\n",
    "    'logits_weights' : tf.Variable(random_normal([n_hidden_2, num_classes]), name='logits_weights')\n",
    "}\n",
    "biases={\n",
    "    'h1_bias' : tf.Variable(tf.zeros([n_hidden_1]), name='h1_bias'),\n",
    "    'h2_bias' : tf.Variable(tf.zeros([n_hidden_2]), name='h2_bias'),\n",
    "    'logits_bias' : tf.Variable(tf.zeros([num_classes]), name='logits_bias')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tf.function\n",
    "def neural_net(x):\n",
    "    with tf.name_scope('Model'):\n",
    "        with tf.name_scope('Hiddenlayer1'):\n",
    "            layer1 = tf.add(tf.matmul(x,weights['h1_weights']), biases['h1_bias'])\n",
    "            layer_1 = tf.nn.sigmoid(layer1)\n",
    "        with tf.name_scope('Hiddenlayer2'):\n",
    "            layer2 = tf.add(tf.matmul(layer1,weights['h2_weights']), biases['h2_bias'])\n",
    "            layer_2 = tf.nn.sigmoid(layer2)\n",
    "        with tf.name_scope('Logitslayer'):\n",
    "            out_layer = tf.matmul(layer_2, weights['logits_weights'])+biases['logits_bias']\n",
    "            out_layer = tf.nn.softmax(out_layer)\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y_pred, y_true):\n",
    "    y_true = tf.one_hot(y_true, depth=num_classes)\n",
    "    y_pred = tf.clip_by_value(y_pred, 1e-9, 1.)\n",
    "    return tf.reduce_mean(-tf.reduce_sum(y_true*tf.math.log(y_pred)))\n",
    "def accuracy(y_pred, y_true):\n",
    "    with tf.name_scope('Accuracy'):\n",
    "        correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.cast(y_true, tf.int64))\n",
    "        return tf.reduce_mean(tf.cast(correct_prediction, tf.float32), axis=-1)\n",
    "with tf.name_scope('Optimizer'):\n",
    "    optimizer = tf.optimizers.SGD(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_optimizatoin(x,y):\n",
    "    with tf.GradientTape() as g:\n",
    "        pred = neural_net(x)\n",
    "        loss = cross_entropy(pred, y)\n",
    "    trainable_variables = list(weigths.values())+list(biases.values())\n",
    "    gradient = g.gradient(loss, trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradient, trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_weights(step):\n",
    "    for w in weights:\n",
    "        #히스토그램을 그릴수 있도록 출력하라.\n",
    "        #replace('_','/') - 저장할때 경로 지정 _를 /로 바꿔주었다.\n",
    "        tf.summary.histogram(w.replace('_','/'), weights[w], step=step)\n",
    "    for b in biases:\n",
    "        tf.summary.histogram(b.replace('_','/'), biases[b], step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_writer = tf.summary.create_file_writer(logs_path)\n",
    "#인스턴스해서 경로를 부여해야한다.\n",
    "#tensorboard --logdir=/tmp/logs \n",
    "#summary는 시각화에 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step :100, loss :610.747803, accuracy :0.078125\n",
      "step :200, loss :614.890991, accuracy :0.113281\n",
      "step :300, loss :629.696411, accuracy :0.085938\n",
      "step :400, loss :615.283142, accuracy :0.117188\n",
      "step :500, loss :608.083679, accuracy :0.097656\n",
      "step :600, loss :610.698120, accuracy :0.109375\n",
      "step :700, loss :621.163086, accuracy :0.082031\n",
      "step :800, loss :613.957581, accuracy :0.074219\n",
      "step :900, loss :610.453491, accuracy :0.093750\n",
      "step :1000, loss :614.234680, accuracy :0.082031\n",
      "step :1100, loss :600.497986, accuracy :0.085938\n",
      "step :1200, loss :616.836060, accuracy :0.097656\n",
      "step :1300, loss :618.177612, accuracy :0.093750\n",
      "step :1400, loss :600.834229, accuracy :0.101562\n",
      "step :1500, loss :615.051086, accuracy :0.089844\n",
      "step :1600, loss :603.918701, accuracy :0.121094\n",
      "step :1700, loss :603.460205, accuracy :0.109375\n",
      "step :1800, loss :604.547058, accuracy :0.125000\n",
      "step :1900, loss :615.200500, accuracy :0.101562\n",
      "step :2000, loss :604.346497, accuracy :0.105469\n",
      "step :2100, loss :595.655884, accuracy :0.117188\n",
      "step :2200, loss :599.881714, accuracy :0.097656\n",
      "step :2300, loss :612.829712, accuracy :0.093750\n",
      "step :2400, loss :600.287842, accuracy :0.117188\n",
      "step :2500, loss :612.017944, accuracy :0.109375\n",
      "step :2600, loss :618.913818, accuracy :0.070312\n",
      "step :2700, loss :606.938354, accuracy :0.113281\n",
      "step :2800, loss :615.500854, accuracy :0.101562\n",
      "step :2900, loss :606.647461, accuracy :0.117188\n",
      "step :3000, loss :611.124268, accuracy :0.113281\n"
     ]
    }
   ],
   "source": [
    "for step, (batch_x, batch_y) in enumerate(train_data.take(training_steps),1):\n",
    "    if step==1:\n",
    "        #step 1일때 시각화모드를 킨다.\n",
    "        #profiler=True - 그때그때 값을 확인할 때 사용\n",
    "        tf.summary.trace_on(graph=True, profiler=True)\n",
    "    run_optimization(batch_x, batch_y)\n",
    "    if step==1:\n",
    "        #trace_export - 출력시작메시지\n",
    "        with summary_writer.as_default():\n",
    "            tf.summary.trace_export(name='trace', step=0, profiler_outdir=logs_path)\n",
    "    if step % display_step == 0:\n",
    "        pred = neural_net(batch_x)\n",
    "        loss = cross_entropy(pred, batch_y)\n",
    "        acc = accuracy(pred, batch_y)\n",
    "        print('step :%i, loss :%f, accuracy :%f'%(step, loss, acc))\n",
    "        #그래프나 summary_write는 default만 작동한다.\n",
    "        #여러개를 각각 만들수 있다. 현재작동하는것은 항상 as_deault()\n",
    "        with summary_writer.as_default():\n",
    "            tf.summary.scalar('loss',loss, step=step) #scalar값으로 저장(단일값)\n",
    "            tf.summary.scalar('accuracy', acc, step=step)\n",
    "            summarize_weights(step) #가중치 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
